{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdval7tUZwdZ"
   },
   "source": [
    "# Groupe Relative Policy Optimization (GRPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CorBhMaiZwdb"
   },
   "source": [
    "Install the Hugging Face libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to fill in the `GRPOTrainer` class. You have two options (and you can do both):\n",
    "* the \"normal GRPO\" with clipped surrogate objective\n",
    "* or the \"vanilla GRPO\" with original objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, level, model_name = \"gpt2\"):\n",
    "        super(Embedder, self).__init__()\n",
    "        if level ==1:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_name).get_input_embeddings().to(device)\n",
    "        else:\n",
    "            #config = AutoConfig.from_pretrained(model_name)\n",
    "            # Create the model from configuration (with random weights)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_name).transformer.to(device)\n",
    "            self.model.wte = nn.Identity()\n",
    "        self.level = level\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.level == 1:\n",
    "            return self.model(input.squeeze(-1))\n",
    "        \n",
    "        out = self.model(inputs_embeds = input)[0][:, -1, :]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Abstract_model(nn.Module):\n",
    "    def __init__(self, level, model_name = \"gpt2\"):\n",
    "        super(Abstract_model, self).__init__()\n",
    "        if level==1:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_name).transformer.to(device)\n",
    "            self.model.wte = nn.Identity()\n",
    "        else:\n",
    "            config = AutoConfig.from_pretrained(model_name)\n",
    "            # Create the model from configuration (with random weights)\n",
    "            self.model = AutoModelForCausalLM.from_config(config).transformer.to(device)\n",
    "            self.model.wte = nn.Identity()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        out = self.model(inputs_embeds = input)[0]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Abstract_level(nn.Module):\n",
    "    def __init__(self, level, model_name = \"gpt2\"):\n",
    "        super(Abstract_level, self).__init__()\n",
    "        self.abstract_size = level\n",
    "        self.model_abstract = Abstract_model(level)\n",
    "\n",
    "        self.level = level\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        output_abstract = self.model_abstract(embeddings) # (batch_size, nseq, embedding_size)\n",
    "        loss = 0\n",
    "        '''\n",
    "        if self.level >1:\n",
    "            output_abstract = output_abstract/torch.linalg.norm(output_abstract, dim=-1).unsqueeze(-1)\n",
    "            loss = - torch.sum(output_abstract[:,:-1].reshape(-1, 768) * embeddings[:,1:].reshape(-1, 768), axis = -1).mean() #+ 0*torch.abs(torch.sum(embeddings[:,1:].reshape(-1, 768)*embeddings[:,:-1].reshape(-1, 768), axis = -1)).mean()\n",
    "        '''\n",
    "        return output_abstract, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token_pred(nn.Module):\n",
    "    def __init__(self, model_name = \"gpt2\"):\n",
    "        super(Token_pred, self).__init__()\n",
    "        #self.layer = AutoModelForCausalLM.from_pretrained(model_name).lm_head.to(device)\n",
    "        self.layer = nn.Linear(768*2, 50257).to(device)\n",
    "        #self.lin2 = nn.Linear(768, 768).to(device) n\n",
    "    \n",
    "    def forward(self, input, condition):\n",
    "        x = torch.cat([input, condition], dim = -1)\n",
    "        #x = input + condition\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldModel_LLM(nn.Module):\n",
    "    def __init__(self, level, use_abstract = True, model_name = \"gpt2\"):\n",
    "        super(WorldModel_LLM, self).__init__()\n",
    "        self.alevel = level\n",
    "        self.aalevel = 4\n",
    "        self.use_abstract = use_abstract\n",
    "\n",
    "        self.token_embedder = Embedder(1)\n",
    "        if use_abstract:\n",
    "            self.lvl1_embedder = Embedder(self.alevel)\n",
    "\n",
    "        self.lvl0_predictor = Abstract_level(1)\n",
    "        if use_abstract:\n",
    "            self.lvl1_predictor = Abstract_level(self.alevel)\n",
    "\n",
    "        self.token_pred = Token_pred()\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.lvl1_embed_save = None\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        nseq = tokens.shape[1]//self.alevel\n",
    "        self.aalevel = nseq\n",
    "        tokens = tokens[:, :tokens.shape[1]//self.alevel*self.alevel]\n",
    "        \n",
    "        tokens = tokens.reshape(tokens.shape[0], -1, self.alevel).reshape(-1, self.alevel) # (batch_size * nseq, alevel)\n",
    "        token_embedding = self.token_embedder(tokens) # (batch_size * nseq, alevel, embedding_size)\n",
    "\n",
    "        if self.use_abstract:\n",
    "            lvl1_embedding = self.lvl1_embedder(token_embedding).reshape(-1, nseq, 768) # (batch_size, nseq, embedding_size)\n",
    "        \n",
    "        if nseq>1:\n",
    "            token_embedding = token_embedding.reshape(-1, nseq, self.alevel, 768)[:,1:].reshape(-1, self.alevel, 768)\n",
    "            tokens = tokens.reshape(-1, nseq, self.alevel)[:,1:].reshape(-1, self.alevel)\n",
    "\n",
    "        token_embedding_pred, _ = self.lvl0_predictor(token_embedding) # (batch_size * nseq, alevel, embedding_size)\n",
    "        loss_abstract = 0\n",
    "        if self.use_abstract:\n",
    "            lvl1_embedding_pred, loss_abstract = self.lvl1_predictor(lvl1_embedding) # (batch_size, nseq, embedding_size)\n",
    "\n",
    "            lvl1_embedding_pred = lvl1_embedding_pred[:,:-1].unsqueeze(2).repeat(1,1, self.alevel, 1).reshape(-1, self.alevel, 768)\n",
    "\n",
    "        if self.use_abstract:\n",
    "            tokens_pred = self.token_pred(token_embedding_pred, lvl1_embedding_pred)\n",
    "        else:\n",
    "            tokens_pred = self.token_pred(token_embedding_pred, 0)\n",
    "        loss_token = self.criterion(tokens_pred[:,:-1].reshape(-1, 50257), tokens[:,1:].reshape(-1))\n",
    "        return tokens_pred, loss_abstract, loss_token\n",
    "    \n",
    "    def generate(self, tokens, ntokens):\n",
    "        with torch.no_grad():\n",
    "            while tokens.shape[1] <= ntokens:\n",
    "                if tokens.shape[1] <= self.alevel:\n",
    "                    token_embedding = self.token_embedder(tokens)\n",
    "                    token_embedding_pred, _ = self.lvl0_predictor(token_embedding)\n",
    "                    token_embedding_pred = token_embedding_pred[:,-1]\n",
    "\n",
    "                    if self.lvl1_embed_save is None:\n",
    "                        lvl1_embedding = self.lvl1_embedder(token_embedding).reshape(-1, 1, 768)\n",
    "                        lvl1_embedding_pred, _ = self.lvl1_predictor(lvl1_embedding)\n",
    "                        lvl1_embedding_pred = lvl1_embedding_pred[:,-1]\n",
    "                        self.lvl1_embed_save = lvl1_embedding_pred\n",
    "                    else:\n",
    "                        lvl1_embedding_pred = self.lvl1_embed_save\n",
    "\n",
    "                else:\n",
    "                    nseq = tokens.shape[1]//self.alevel\n",
    "                    reste = tokens.shape[1]%self.alevel\n",
    "                    token_embedding = self.token_embedder(tokens)\n",
    "                    token_embedding_pred, _ = self.lvl0_predictor(token_embedding[:, 1-self.alevel:])\n",
    "                    token_embedding_pred = token_embedding_pred[:,-1]\n",
    "\n",
    "                    if reste==0:\n",
    "                        embed_for_lvl1 = token_embedding[:, :tokens.shape[1]//self.alevel*self.alevel].reshape(tokens.shape[0], -1, self.alevel, 768).reshape(-1, self.alevel, 768)\n",
    "                        lvl1_embedding = self.lvl1_embedder(embed_for_lvl1).reshape(-1, nseq, 768)\n",
    "                        lvl1_embedding_pred, _ = self.lvl1_predictor(lvl1_embedding[:, -self.aalevel-1:])\n",
    "                        lvl1_embedding_pred = lvl1_embedding_pred[:,-2]\n",
    "                        self.lvl1_embed_save = lvl1_embedding_pred\n",
    "                    else:\n",
    "                        lvl1_embedding_pred = self.lvl1_embed_save\n",
    "\n",
    "                tokens_pred = self.token_pred(token_embedding_pred, lvl1_embedding_pred)\n",
    "                next_token = torch.multinomial(tokens_pred.softmax(-1), 1)\n",
    "                tokens = torch.cat([tokens, next_token], dim = 1)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an example of me and of\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "print(input_ids.shape)\n",
    "\n",
    "wmllm = WorldModel_LLM(16).to(device)\n",
    "with torch.no_grad():\n",
    "    #op = wmllm(input_ids)\n",
    "    op = wmllm.generate(input_ids, 15)\n",
    "    tokenizer.decode(op[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmllm.aalevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 17])\n",
      "torch.Size([1, 17, 50257]) tensor(4.2338, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an example sentence. What do you think of this simple fact: i \"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "print(input_ids.shape)\n",
    "\n",
    "wmllm = Simple_LLM().to(device)\n",
    "res,_, loss1 = wmllm(input_ids)\n",
    "print(res.shape, loss1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is an example sentence. What do you think of this simple fact: i \\xa0have a \\xa0a \\xa0a \\xa0a \\xa0a \\xa0a \\xa0a \\xa0a \\xa0a \\xa0a \\xa0a'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op = wmllm.generate(input_ids, 50)\n",
    "tokenizer.decode(op[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr. speaker, mr. president, and distinguished members of the house and senate, honored guests, and fellow citizens: less than 3 weeks ago, i joined you on the west front of this very building and, looking over the monuments to our proud past, offered you my hand in filling the next page of american history with a story of extended prosperity and continued peace. and tonight i'm back to offer you my plans as well. the hand remains extended; the sleeves are rolled up; america is waiting; and now we must produce. together, we can build a better america.\n",
      "it is comforting to return to this historic chamber. here, 22 years ago, i first raised my hand to be sworn into public life. so, tonight i feel as if i'm returning home to friends. and i intend, in the months and years to come, to give you what friends deserve: frankness, respect, and my best judgment about ways to improve america's future. in return, i ask for an honest commitment to our common mission of progress. if we seize the opport\n"
     ]
    }
   ],
   "source": [
    "with open('llm_mva/sotu.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('  ', '\\n')\n",
    "\n",
    "\n",
    "text = text.lower()\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (260738 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([260738])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tokenizer.encode(text, return_tensors='pt').squeeze()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([260738]),\n",
       " tensor([43395,    13, 10834,    11,   285,    81,    13,  1893,    11,   290,\n",
       "         18876,  1866,   286,   262,  2156,   290, 34548,    11, 21014, 10650,\n",
       "            11,   290,  5891,  4290,    25,  1342,   621,   513,  2745,  2084,\n",
       "            11,  1312,  5399,   345,   319,   262,  7421,  2166,   286,   428,\n",
       "           845,  2615,   290,    11,  2045,   625,   262, 28814,   284,   674]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "lendata = train_data.shape[0]\n",
    "data.shape, data[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 512\n",
    "batch_size = 8\n",
    " \n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_length - 1, (batch_size,))\n",
    "    X = torch.stack([data[i:i+context_length] for i in ix])\n",
    "    return X\n",
    "\n",
    "X = get_batch(\"train\")\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  validation(model):\n",
    "    # Define hyperparameters\n",
    "    epoch = 1\n",
    "\n",
    "    # Training loop\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        num =5\n",
    "        for i in range(num):\n",
    "            # Get batch\n",
    "            X = get_batch('val')\n",
    "            \n",
    "            # Forward pass\n",
    "            output, loss_abstract, loss_token = model(X.to(device))\n",
    "            loss = loss_abstract + loss_token\n",
    "            \n",
    "            total_loss += loss.item()  \n",
    "\n",
    "        avg_loss = total_loss / num \n",
    "        print(f\"Validation: {epoch+1}, Average Loss: {     avg_loss}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WorldModel_LLM(32).to(device)\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#model.load_state_dict(torch.load(\"model.pt\"))\n",
    "best_loss = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "Epoch: 1, Step: 0, loss_token: 4.844322681427002\n",
      "Epoch: 1, Step: 1, loss_token: 4.419561862945557\n",
      "Epoch: 1, Step: 2, loss_token: 4.005755424499512\n",
      "Epoch: 1, Step: 3, loss_token: 4.0972371101379395\n",
      "Epoch: 1, Step: 4, loss_token: 3.9675912857055664\n",
      "Epoch: 1, Step: 5, loss_token: 3.9356353282928467\n",
      "Epoch: 1, Step: 6, loss_token: 3.8363096714019775\n",
      "Epoch: 1, Step: 7, loss_token: 4.015597820281982\n",
      "Epoch: 1, Step: 8, loss_token: 3.6146605014801025\n",
      "Epoch: 1, Step: 9, loss_token: 3.837869882583618\n",
      "Epoch: 1, Step: 10, loss_token: 4.036443710327148\n",
      "Epoch: 1, Step: 11, loss_token: 3.822021484375\n",
      "Epoch: 1, Step: 12, loss_token: 3.77701997756958\n",
      "Epoch: 1, Step: 13, loss_token: 3.7468626499176025\n",
      "Epoch: 1, Step: 14, loss_token: 3.6518428325653076\n",
      "Epoch: 1, Step: 15, loss_token: 3.598021984100342\n",
      "Epoch: 1, Step: 16, loss_token: 3.7293009757995605\n",
      "Epoch: 1, Step: 17, loss_token: 3.5807290077209473\n",
      "Epoch: 1, Step: 18, loss_token: 3.741987943649292\n",
      "Epoch: 1, Step: 19, loss_token: 3.6366846561431885\n",
      "Epoch: 1, Step: 20, loss_token: 3.6329455375671387\n",
      "Epoch: 1, Step: 21, loss_token: 3.598843574523926\n",
      "Epoch: 1, Step: 22, loss_token: 3.6199567317962646\n",
      "Epoch: 1, Step: 23, loss_token: 3.662126064300537\n",
      "Epoch: 1, Step: 24, loss_token: 3.5228421688079834\n",
      "Epoch: 1, Step: 25, loss_token: 3.5685713291168213\n",
      "Epoch: 1, Step: 26, loss_token: 3.6914172172546387\n",
      "Epoch: 1, Step: 27, loss_token: 3.507086992263794\n",
      "Epoch: 1, Step: 28, loss_token: 3.4912171363830566\n",
      "Epoch: 1, Step: 29, loss_token: 3.5488831996917725\n",
      "Epoch: 1, Step: 30, loss_token: 3.7172799110412598\n",
      "Epoch: 1, Step: 31, loss_token: 3.544947624206543\n",
      "Epoch: 1, Step: 32, loss_token: 3.626899242401123\n",
      "Epoch: 1, Step: 33, loss_token: 3.6008405685424805\n",
      "Epoch: 1, Step: 34, loss_token: 3.5306220054626465\n",
      "Epoch: 1, Step: 35, loss_token: 3.610960006713867\n",
      "Epoch: 1, Step: 36, loss_token: 3.4518179893493652\n",
      "Epoch: 1, Step: 37, loss_token: 3.615417718887329\n",
      "Epoch: 1, Step: 38, loss_token: 3.440828800201416\n",
      "Epoch: 1, Step: 39, loss_token: 3.4861154556274414\n",
      "Epoch: 1, Step: 40, loss_token: 3.4383621215820312\n",
      "Epoch: 1, Step: 41, loss_token: 3.4559035301208496\n",
      "Epoch: 1, Step: 42, loss_token: 3.390681028366089\n",
      "Epoch: 1, Step: 43, loss_token: 3.483930826187134\n",
      "Epoch: 1, Step: 44, loss_token: 3.623725652694702\n",
      "Epoch: 1, Step: 45, loss_token: 3.4713242053985596\n",
      "Epoch: 1, Step: 46, loss_token: 3.4096872806549072\n",
      "Epoch: 1, Step: 47, loss_token: 3.548872470855713\n",
      "Epoch: 1, Step: 48, loss_token: 3.5387167930603027\n",
      "Epoch: 1, Step: 49, loss_token: 3.501352548599243\n",
      "Epoch: 1, Step: 50, loss_token: 3.483776092529297\n",
      "Epoch: 1, Step: 51, loss_token: 3.36580228805542\n",
      "Epoch: 1, Step: 52, loss_token: 3.4259660243988037\n",
      "Epoch: 1, Step: 53, loss_token: 3.5148184299468994\n",
      "Epoch: 1, Step: 54, loss_token: 3.598071336746216\n",
      "Epoch: 1, Step: 55, loss_token: 3.573310375213623\n",
      "Epoch: 1, Step: 56, loss_token: 3.482257127761841\n",
      "Epoch: 1, Step: 57, loss_token: 3.3663625717163086\n",
      "Epoch: 1, Step: 58, loss_token: 3.409782886505127\n",
      "Epoch: 1, Step: 59, loss_token: 3.3401126861572266\n",
      "Epoch: 1, Step: 60, loss_token: 3.3313021659851074\n",
      "Epoch: 1, Step: 61, loss_token: 3.5819578170776367\n",
      "Epoch: 1, Step: 62, loss_token: 3.3051583766937256\n",
      "Epoch: 1, Step: 63, loss_token: 3.4501140117645264\n",
      "Epoch: 1, Step: 64, loss_token: 3.374673843383789\n",
      "Epoch: 1, Step: 65, loss_token: 3.4531917572021484\n",
      "Epoch: 1, Step: 66, loss_token: 3.3104357719421387\n",
      "Epoch: 1, Step: 67, loss_token: 3.510960578918457\n",
      "Epoch: 1, Step: 68, loss_token: 3.3456501960754395\n",
      "Epoch: 1, Step: 69, loss_token: 3.610328197479248\n",
      "Epoch: 1, Step: 70, loss_token: 3.378139019012451\n",
      "Epoch: 1, Step: 71, loss_token: 3.6121015548706055\n",
      "Epoch: 1, Step: 72, loss_token: 3.3633017539978027\n",
      "Epoch: 1, Step: 73, loss_token: 3.335237741470337\n",
      "Epoch: 1, Step: 74, loss_token: 3.4228434562683105\n",
      "Epoch: 1, Step: 75, loss_token: 3.376969575881958\n",
      "Epoch: 1, Average Loss: 3.5993455021004928\n",
      "Validation: 2, Average Loss: 3.6095906257629395\n",
      "Model saved\n",
      "Epoch: 2, Step: 0, loss_token: 3.333707094192505\n",
      "Epoch: 2, Step: 1, loss_token: 3.424110174179077\n",
      "Epoch: 2, Step: 2, loss_token: 3.5381367206573486\n",
      "Epoch: 2, Step: 3, loss_token: 3.350536823272705\n",
      "Epoch: 2, Step: 4, loss_token: 3.5045859813690186\n",
      "Epoch: 2, Step: 5, loss_token: 3.4270715713500977\n",
      "Epoch: 2, Step: 6, loss_token: 3.4095518589019775\n",
      "Epoch: 2, Step: 7, loss_token: 3.3311660289764404\n",
      "Epoch: 2, Step: 8, loss_token: 3.2310476303100586\n",
      "Epoch: 2, Step: 9, loss_token: 3.4971041679382324\n",
      "Epoch: 2, Step: 10, loss_token: 3.439499855041504\n",
      "Epoch: 2, Step: 11, loss_token: 3.2989113330841064\n",
      "Epoch: 2, Step: 12, loss_token: 3.2417781352996826\n",
      "Epoch: 2, Step: 13, loss_token: 3.1832070350646973\n",
      "Epoch: 2, Step: 14, loss_token: 3.4197611808776855\n",
      "Epoch: 2, Step: 15, loss_token: 3.2871897220611572\n",
      "Epoch: 2, Step: 16, loss_token: 3.4757354259490967\n",
      "Epoch: 2, Step: 17, loss_token: 3.3722984790802\n",
      "Epoch: 2, Step: 18, loss_token: 3.2996623516082764\n",
      "Epoch: 2, Step: 19, loss_token: 3.2700393199920654\n",
      "Epoch: 2, Step: 20, loss_token: 3.1837215423583984\n",
      "Epoch: 2, Step: 21, loss_token: 3.2450037002563477\n",
      "Epoch: 2, Step: 22, loss_token: 3.3107290267944336\n",
      "Epoch: 2, Step: 23, loss_token: 3.3122339248657227\n",
      "Epoch: 2, Step: 24, loss_token: 3.372866153717041\n",
      "Epoch: 2, Step: 25, loss_token: 3.2332112789154053\n",
      "Epoch: 2, Step: 26, loss_token: 3.258598566055298\n",
      "Epoch: 2, Step: 27, loss_token: 3.3287642002105713\n",
      "Epoch: 2, Step: 28, loss_token: 3.3786277770996094\n",
      "Epoch: 2, Step: 29, loss_token: 3.4447429180145264\n",
      "Epoch: 2, Step: 30, loss_token: 3.462390899658203\n",
      "Epoch: 2, Step: 31, loss_token: 3.43167781829834\n",
      "Epoch: 2, Step: 32, loss_token: 3.3412857055664062\n",
      "Epoch: 2, Step: 33, loss_token: 3.300351619720459\n",
      "Epoch: 2, Step: 34, loss_token: 3.2264978885650635\n",
      "Epoch: 2, Step: 35, loss_token: 3.277674674987793\n",
      "Epoch: 2, Step: 36, loss_token: 3.4148311614990234\n",
      "Epoch: 2, Step: 37, loss_token: 3.424753427505493\n",
      "Epoch: 2, Step: 38, loss_token: 3.3335630893707275\n",
      "Epoch: 2, Step: 39, loss_token: 3.2927117347717285\n",
      "Epoch: 2, Step: 40, loss_token: 3.302483320236206\n",
      "Epoch: 2, Step: 41, loss_token: 3.2243399620056152\n",
      "Epoch: 2, Step: 42, loss_token: 3.1675729751586914\n",
      "Epoch: 2, Step: 43, loss_token: 3.3149445056915283\n",
      "Epoch: 2, Step: 44, loss_token: 3.344759941101074\n",
      "Epoch: 2, Step: 45, loss_token: 3.17965030670166\n",
      "Epoch: 2, Step: 46, loss_token: 3.3116023540496826\n",
      "Epoch: 2, Step: 47, loss_token: 3.317918300628662\n",
      "Epoch: 2, Step: 48, loss_token: 3.232534408569336\n",
      "Epoch: 2, Step: 49, loss_token: 3.226090908050537\n",
      "Epoch: 2, Step: 50, loss_token: 3.340785026550293\n",
      "Epoch: 2, Step: 51, loss_token: 3.2839455604553223\n",
      "Epoch: 2, Step: 52, loss_token: 3.1878833770751953\n",
      "Epoch: 2, Step: 53, loss_token: 3.1700546741485596\n",
      "Epoch: 2, Step: 54, loss_token: 3.2332780361175537\n",
      "Epoch: 2, Step: 55, loss_token: 3.484543561935425\n",
      "Epoch: 2, Step: 56, loss_token: 3.2761733531951904\n",
      "Epoch: 2, Step: 57, loss_token: 3.3976244926452637\n",
      "Epoch: 2, Step: 58, loss_token: 3.162198781967163\n",
      "Epoch: 2, Step: 59, loss_token: 3.150996208190918\n",
      "Epoch: 2, Step: 60, loss_token: 3.1831891536712646\n",
      "Epoch: 2, Step: 61, loss_token: 3.4218223094940186\n",
      "Epoch: 2, Step: 62, loss_token: 3.3481287956237793\n",
      "Epoch: 2, Step: 63, loss_token: 3.1604857444763184\n",
      "Epoch: 2, Step: 64, loss_token: 3.235353469848633\n",
      "Epoch: 2, Step: 65, loss_token: 3.240328550338745\n",
      "Epoch: 2, Step: 66, loss_token: 3.1484031677246094\n",
      "Epoch: 2, Step: 67, loss_token: 3.343914031982422\n",
      "Epoch: 2, Step: 68, loss_token: 3.246521234512329\n",
      "Epoch: 2, Step: 69, loss_token: 3.2358107566833496\n",
      "Epoch: 2, Step: 70, loss_token: 3.1915934085845947\n",
      "Epoch: 2, Step: 71, loss_token: 3.236335039138794\n",
      "Epoch: 2, Step: 72, loss_token: 3.178936243057251\n",
      "Epoch: 2, Step: 73, loss_token: 3.2086970806121826\n",
      "Epoch: 2, Step: 74, loss_token: 3.270413875579834\n",
      "Epoch: 2, Step: 75, loss_token: 3.1806440353393555\n",
      "Epoch: 2, Average Loss: 3.3032801966918144\n",
      "Validation: 2, Average Loss: 3.547338342666626\n",
      "Model saved\n",
      "Epoch: 3, Step: 0, loss_token: 3.32802152633667\n",
      "Epoch: 3, Step: 1, loss_token: 3.3924789428710938\n",
      "Epoch: 3, Step: 2, loss_token: 3.204097270965576\n",
      "Epoch: 3, Step: 3, loss_token: 3.5110435485839844\n",
      "Epoch: 3, Step: 4, loss_token: 3.0646111965179443\n",
      "Epoch: 3, Step: 5, loss_token: 3.084519386291504\n",
      "Epoch: 3, Step: 6, loss_token: 3.1747114658355713\n",
      "Epoch: 3, Step: 7, loss_token: 3.3230769634246826\n",
      "Epoch: 3, Step: 8, loss_token: 3.096754789352417\n",
      "Epoch: 3, Step: 9, loss_token: 3.1211836338043213\n",
      "Epoch: 3, Step: 10, loss_token: 3.1872031688690186\n",
      "Epoch: 3, Step: 11, loss_token: 3.519172191619873\n",
      "Epoch: 3, Step: 12, loss_token: 3.226818799972534\n",
      "Epoch: 3, Step: 13, loss_token: 3.1357085704803467\n",
      "Epoch: 3, Step: 14, loss_token: 3.469125509262085\n",
      "Epoch: 3, Step: 15, loss_token: 3.1835103034973145\n",
      "Epoch: 3, Step: 16, loss_token: 3.2553794384002686\n",
      "Epoch: 3, Step: 17, loss_token: 3.2253828048706055\n",
      "Epoch: 3, Step: 18, loss_token: 3.2562921047210693\n",
      "Epoch: 3, Step: 19, loss_token: 3.3040621280670166\n",
      "Epoch: 3, Step: 20, loss_token: 3.0633316040039062\n",
      "Epoch: 3, Step: 21, loss_token: 3.278233051300049\n",
      "Epoch: 3, Step: 22, loss_token: 3.302622079849243\n",
      "Epoch: 3, Step: 23, loss_token: 3.184628486633301\n",
      "Epoch: 3, Step: 24, loss_token: 3.1593146324157715\n",
      "Epoch: 3, Step: 25, loss_token: 3.131674289703369\n",
      "Epoch: 3, Step: 26, loss_token: 3.109571695327759\n",
      "Epoch: 3, Step: 27, loss_token: 3.1011998653411865\n",
      "Epoch: 3, Step: 28, loss_token: 3.087351083755493\n",
      "Epoch: 3, Step: 29, loss_token: 3.097856283187866\n",
      "Epoch: 3, Step: 30, loss_token: 3.0223190784454346\n",
      "Epoch: 3, Step: 31, loss_token: 3.0087890625\n",
      "Epoch: 3, Step: 32, loss_token: 3.132310152053833\n",
      "Epoch: 3, Step: 33, loss_token: 3.083857297897339\n",
      "Epoch: 3, Step: 34, loss_token: 3.1794474124908447\n",
      "Epoch: 3, Step: 35, loss_token: 3.2208542823791504\n",
      "Epoch: 3, Step: 36, loss_token: 3.1808152198791504\n",
      "Epoch: 3, Step: 37, loss_token: 3.234429121017456\n",
      "Epoch: 3, Step: 38, loss_token: 3.195788621902466\n",
      "Epoch: 3, Step: 39, loss_token: 3.0123133659362793\n",
      "Epoch: 3, Step: 40, loss_token: 3.2001192569732666\n",
      "Epoch: 3, Step: 41, loss_token: 3.2938408851623535\n",
      "Epoch: 3, Step: 42, loss_token: 3.145003318786621\n",
      "Epoch: 3, Step: 43, loss_token: 3.1830532550811768\n",
      "Epoch: 3, Step: 44, loss_token: 3.1864781379699707\n",
      "Epoch: 3, Step: 45, loss_token: 3.0783278942108154\n",
      "Epoch: 3, Step: 46, loss_token: 3.0358726978302\n",
      "Epoch: 3, Step: 47, loss_token: 3.0537986755371094\n",
      "Epoch: 3, Step: 48, loss_token: 3.522244930267334\n",
      "Epoch: 3, Step: 49, loss_token: 3.1594278812408447\n",
      "Epoch: 3, Step: 50, loss_token: 3.2893669605255127\n",
      "Epoch: 3, Step: 51, loss_token: 3.2341339588165283\n",
      "Epoch: 3, Step: 52, loss_token: 3.044102907180786\n",
      "Epoch: 3, Step: 53, loss_token: 3.0893137454986572\n",
      "Epoch: 3, Step: 54, loss_token: 3.0884246826171875\n",
      "Epoch: 3, Step: 55, loss_token: 3.167825698852539\n",
      "Epoch: 3, Step: 56, loss_token: 3.2260212898254395\n",
      "Epoch: 3, Step: 57, loss_token: 2.988266706466675\n",
      "Epoch: 3, Step: 58, loss_token: 3.0425474643707275\n",
      "Epoch: 3, Step: 59, loss_token: 3.0286924839019775\n",
      "Epoch: 3, Step: 60, loss_token: 3.1057240962982178\n",
      "Epoch: 3, Step: 61, loss_token: 3.0344386100769043\n",
      "Epoch: 3, Step: 62, loss_token: 3.147456645965576\n",
      "Epoch: 3, Step: 63, loss_token: 3.3089568614959717\n",
      "Epoch: 3, Step: 64, loss_token: 3.2051682472229004\n",
      "Epoch: 3, Step: 65, loss_token: 3.060116767883301\n",
      "Epoch: 3, Step: 66, loss_token: 3.083611011505127\n",
      "Epoch: 3, Step: 67, loss_token: 3.1217682361602783\n",
      "Epoch: 3, Step: 68, loss_token: 3.120478630065918\n",
      "Epoch: 3, Step: 69, loss_token: 3.097334384918213\n",
      "Epoch: 3, Step: 70, loss_token: 3.092090129852295\n",
      "Epoch: 3, Step: 71, loss_token: 3.026460886001587\n",
      "Epoch: 3, Step: 72, loss_token: 3.267848253250122\n",
      "Epoch: 3, Step: 73, loss_token: 2.9953813552856445\n",
      "Epoch: 3, Step: 74, loss_token: 3.181063413619995\n",
      "Epoch: 3, Step: 75, loss_token: 3.2108263969421387\n",
      "Epoch: 3, Average Loss: 3.170597989308207\n",
      "Validation: 2, Average Loss: 3.585718297958374\n",
      "Epoch: 4, Step: 0, loss_token: 3.113712787628174\n",
      "Epoch: 4, Step: 1, loss_token: 3.023094654083252\n",
      "Epoch: 4, Step: 2, loss_token: 3.0358049869537354\n",
      "Epoch: 4, Step: 3, loss_token: 3.124354124069214\n",
      "Epoch: 4, Step: 4, loss_token: 3.1074628829956055\n",
      "Epoch: 4, Step: 5, loss_token: 3.113589286804199\n",
      "Epoch: 4, Step: 6, loss_token: 3.272433042526245\n",
      "Epoch: 4, Step: 7, loss_token: 2.92143177986145\n",
      "Epoch: 4, Step: 8, loss_token: 2.962221145629883\n",
      "Epoch: 4, Step: 9, loss_token: 3.0281143188476562\n",
      "Epoch: 4, Step: 10, loss_token: 3.05686616897583\n",
      "Epoch: 4, Step: 11, loss_token: 3.054781198501587\n",
      "Epoch: 4, Step: 12, loss_token: 3.2711830139160156\n",
      "Epoch: 4, Step: 13, loss_token: 3.0877246856689453\n",
      "Epoch: 4, Step: 14, loss_token: 2.990161657333374\n",
      "Epoch: 4, Step: 15, loss_token: 2.899463176727295\n",
      "Epoch: 4, Step: 16, loss_token: 3.0398826599121094\n",
      "Epoch: 4, Step: 17, loss_token: 2.9742014408111572\n",
      "Epoch: 4, Step: 18, loss_token: 2.9648468494415283\n",
      "Epoch: 4, Step: 19, loss_token: 3.1162753105163574\n",
      "Epoch: 4, Step: 20, loss_token: 3.1180243492126465\n",
      "Epoch: 4, Step: 21, loss_token: 2.9370150566101074\n",
      "Epoch: 4, Step: 22, loss_token: 3.037781000137329\n",
      "Epoch: 4, Step: 23, loss_token: 3.113131284713745\n",
      "Epoch: 4, Step: 24, loss_token: 3.011583089828491\n",
      "Epoch: 4, Step: 25, loss_token: 3.0036516189575195\n",
      "Epoch: 4, Step: 26, loss_token: 2.9548180103302\n",
      "Epoch: 4, Step: 27, loss_token: 3.184091329574585\n",
      "Epoch: 4, Step: 28, loss_token: 3.068265438079834\n",
      "Epoch: 4, Step: 29, loss_token: 2.979696273803711\n",
      "Epoch: 4, Step: 30, loss_token: 3.1023879051208496\n",
      "Epoch: 4, Step: 31, loss_token: 3.232558488845825\n",
      "Epoch: 4, Step: 32, loss_token: 2.9504270553588867\n",
      "Epoch: 4, Step: 33, loss_token: 3.0472660064697266\n",
      "Epoch: 4, Step: 34, loss_token: 2.9692983627319336\n",
      "Epoch: 4, Step: 35, loss_token: 2.9066781997680664\n",
      "Epoch: 4, Step: 36, loss_token: 3.1296465396881104\n",
      "Epoch: 4, Step: 37, loss_token: 3.039060354232788\n",
      "Epoch: 4, Step: 38, loss_token: 3.089784622192383\n",
      "Epoch: 4, Step: 39, loss_token: 3.0838310718536377\n",
      "Epoch: 4, Step: 40, loss_token: 2.827263593673706\n",
      "Epoch: 4, Step: 41, loss_token: 2.9533631801605225\n",
      "Epoch: 4, Step: 42, loss_token: 2.986443281173706\n",
      "Epoch: 4, Step: 43, loss_token: 2.952695608139038\n",
      "Epoch: 4, Step: 44, loss_token: 3.2188727855682373\n",
      "Epoch: 4, Step: 45, loss_token: 2.887061595916748\n",
      "Epoch: 4, Step: 46, loss_token: 2.9340107440948486\n",
      "Epoch: 4, Step: 47, loss_token: 2.998595952987671\n",
      "Epoch: 4, Step: 48, loss_token: 3.02040696144104\n",
      "Epoch: 4, Step: 49, loss_token: 2.9591476917266846\n",
      "Epoch: 4, Step: 50, loss_token: 2.8209598064422607\n",
      "Epoch: 4, Step: 51, loss_token: 2.952864646911621\n",
      "Epoch: 4, Step: 52, loss_token: 2.9798007011413574\n",
      "Epoch: 4, Step: 53, loss_token: 2.987448215484619\n",
      "Epoch: 4, Step: 54, loss_token: 3.0659403800964355\n",
      "Epoch: 4, Step: 55, loss_token: 3.055159330368042\n",
      "Epoch: 4, Step: 56, loss_token: 2.941831111907959\n",
      "Epoch: 4, Step: 57, loss_token: 2.959027051925659\n",
      "Epoch: 4, Step: 58, loss_token: 2.9572553634643555\n",
      "Epoch: 4, Step: 59, loss_token: 3.1034152507781982\n",
      "Epoch: 4, Step: 60, loss_token: 2.952317714691162\n",
      "Epoch: 4, Step: 61, loss_token: 2.9819788932800293\n",
      "Epoch: 4, Step: 62, loss_token: 3.026191234588623\n",
      "Epoch: 4, Step: 63, loss_token: 3.0805063247680664\n",
      "Epoch: 4, Step: 64, loss_token: 3.0211966037750244\n",
      "Epoch: 4, Step: 65, loss_token: 2.9730632305145264\n",
      "Epoch: 4, Step: 66, loss_token: 2.88244891166687\n",
      "Epoch: 4, Step: 67, loss_token: 3.0375914573669434\n",
      "Epoch: 4, Step: 68, loss_token: 2.859339475631714\n",
      "Epoch: 4, Step: 69, loss_token: 2.8265013694763184\n",
      "Epoch: 4, Step: 70, loss_token: 3.028315305709839\n",
      "Epoch: 4, Step: 71, loss_token: 2.8247907161712646\n",
      "Epoch: 4, Step: 72, loss_token: 3.133897066116333\n",
      "Epoch: 4, Step: 73, loss_token: 2.898052215576172\n",
      "Epoch: 4, Step: 74, loss_token: 2.902440309524536\n",
      "Epoch: 4, Step: 75, loss_token: 2.9899871349334717\n",
      "Epoch: 4, Average Loss: 3.0144313479724683\n",
      "Validation: 2, Average Loss: 3.5503773212432863\n",
      "Epoch: 5, Step: 0, loss_token: 2.9580671787261963\n",
      "Epoch: 5, Step: 1, loss_token: 2.8635060787200928\n",
      "Epoch: 5, Step: 2, loss_token: 2.791900873184204\n",
      "Epoch: 5, Step: 3, loss_token: 2.963675022125244\n",
      "Epoch: 5, Step: 4, loss_token: 2.905061960220337\n",
      "Epoch: 5, Step: 5, loss_token: 2.815459728240967\n",
      "Epoch: 5, Step: 6, loss_token: 2.94901967048645\n",
      "Epoch: 5, Step: 7, loss_token: 2.7911317348480225\n",
      "Epoch: 5, Step: 8, loss_token: 2.9850847721099854\n",
      "Epoch: 5, Step: 9, loss_token: 2.9971654415130615\n",
      "Epoch: 5, Step: 10, loss_token: 2.9326701164245605\n",
      "Epoch: 5, Step: 11, loss_token: 3.0523929595947266\n",
      "Epoch: 5, Step: 12, loss_token: 2.853268623352051\n",
      "Epoch: 5, Step: 13, loss_token: 2.9873926639556885\n",
      "Epoch: 5, Step: 14, loss_token: 2.8060550689697266\n",
      "Epoch: 5, Step: 15, loss_token: 2.846125841140747\n",
      "Epoch: 5, Step: 16, loss_token: 3.0860602855682373\n",
      "Epoch: 5, Step: 17, loss_token: 2.943990707397461\n",
      "Epoch: 5, Step: 18, loss_token: 2.957225799560547\n",
      "Epoch: 5, Step: 19, loss_token: 2.8945095539093018\n",
      "Epoch: 5, Step: 20, loss_token: 2.8773443698883057\n",
      "Epoch: 5, Step: 21, loss_token: 2.909506320953369\n",
      "Epoch: 5, Step: 22, loss_token: 2.8754875659942627\n",
      "Epoch: 5, Step: 23, loss_token: 3.0066823959350586\n",
      "Epoch: 5, Step: 24, loss_token: 2.9569571018218994\n",
      "Epoch: 5, Step: 25, loss_token: 2.9342076778411865\n",
      "Epoch: 5, Step: 26, loss_token: 2.863913059234619\n",
      "Epoch: 5, Step: 27, loss_token: 2.857398748397827\n",
      "Epoch: 5, Step: 28, loss_token: 2.837261438369751\n",
      "Epoch: 5, Step: 29, loss_token: 2.8624987602233887\n",
      "Epoch: 5, Step: 30, loss_token: 3.104339122772217\n",
      "Epoch: 5, Step: 31, loss_token: 2.823361873626709\n",
      "Epoch: 5, Step: 32, loss_token: 2.9732894897460938\n",
      "Epoch: 5, Step: 33, loss_token: 3.0404443740844727\n",
      "Epoch: 5, Step: 34, loss_token: 2.8704447746276855\n",
      "Epoch: 5, Step: 35, loss_token: 2.895275115966797\n",
      "Epoch: 5, Step: 36, loss_token: 2.9497621059417725\n",
      "Epoch: 5, Step: 37, loss_token: 2.9585540294647217\n",
      "Epoch: 5, Step: 38, loss_token: 2.9966037273406982\n",
      "Epoch: 5, Step: 39, loss_token: 2.7841885089874268\n",
      "Epoch: 5, Step: 40, loss_token: 2.9534378051757812\n",
      "Epoch: 5, Step: 41, loss_token: 2.9461143016815186\n",
      "Epoch: 5, Step: 42, loss_token: 2.846120834350586\n",
      "Epoch: 5, Step: 43, loss_token: 2.7911033630371094\n",
      "Epoch: 5, Step: 44, loss_token: 2.91695499420166\n",
      "Epoch: 5, Step: 45, loss_token: 3.145900011062622\n",
      "Epoch: 5, Step: 46, loss_token: 2.7767553329467773\n",
      "Epoch: 5, Step: 47, loss_token: 2.826244831085205\n",
      "Epoch: 5, Step: 48, loss_token: 2.8011081218719482\n",
      "Epoch: 5, Step: 49, loss_token: 2.995553970336914\n",
      "Epoch: 5, Step: 50, loss_token: 2.757127046585083\n",
      "Epoch: 5, Step: 51, loss_token: 3.0661752223968506\n",
      "Epoch: 5, Step: 52, loss_token: 2.6311628818511963\n",
      "Epoch: 5, Step: 53, loss_token: 3.145927667617798\n",
      "Epoch: 5, Step: 54, loss_token: 2.7994959354400635\n",
      "Epoch: 5, Step: 55, loss_token: 2.7556402683258057\n",
      "Epoch: 5, Step: 56, loss_token: 2.9007856845855713\n",
      "Epoch: 5, Step: 57, loss_token: 2.861232280731201\n",
      "Epoch: 5, Step: 58, loss_token: 3.024740695953369\n",
      "Epoch: 5, Step: 59, loss_token: 2.862990617752075\n",
      "Epoch: 5, Step: 60, loss_token: 2.7943947315216064\n",
      "Epoch: 5, Step: 61, loss_token: 2.8348028659820557\n",
      "Epoch: 5, Step: 62, loss_token: 2.748962640762329\n",
      "Epoch: 5, Step: 63, loss_token: 2.9679596424102783\n",
      "Epoch: 5, Step: 64, loss_token: 2.8293182849884033\n",
      "Epoch: 5, Step: 65, loss_token: 2.83127498626709\n",
      "Epoch: 5, Step: 66, loss_token: 3.0525028705596924\n",
      "Epoch: 5, Step: 67, loss_token: 2.740494728088379\n",
      "Epoch: 5, Step: 68, loss_token: 2.8235175609588623\n",
      "Epoch: 5, Step: 69, loss_token: 2.730503559112549\n",
      "Epoch: 5, Step: 70, loss_token: 2.8472232818603516\n",
      "Epoch: 5, Step: 71, loss_token: 2.7748608589172363\n",
      "Epoch: 5, Step: 72, loss_token: 2.8690268993377686\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "epochs = 6\n",
    "\n",
    "# Define optimizer\n",
    "\n",
    "print(int(1.33*lendata/(batch_size*context_length)))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i in range(0,int(1.33*lendata/(batch_size*context_length))):\n",
    "        # Get batch\n",
    "        X = get_batch('train')\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output, loss_abstract, loss_token = model(X.to(device))\n",
    "        loss = loss_abstract + loss_token\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()  \n",
    "         \n",
    "        if i % 1 == 0:\n",
    "            print(f\"Epoch: {epoch+1}, Step: {i}, loss_token: {loss_token.item()}\")\n",
    "    \n",
    "    avg_loss = total_loss/int(1.33*lendata/(batch_size*context_length))\n",
    "    print(f\"Epoch: {epoch+1}, Average Loss: {     avg_loss}\")\n",
    "\n",
    "    if epoch > -1:\n",
    "        val_loss = validation(model)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"model.pt\")\n",
    "            print(\"Model saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will explain the meaning of life. The meaning of life is to serve it by serving freedom. and life in a free country is nothing but gratitude for a country which has serviced us in spite of the pressures of austerity and unemployment.\n",
      "well, yes. by the time too are we ready. so let's back to the business of agriculture. as i said, the livestock emissions caps have been cut--a's focus. all of us at home today will agree. we must take action now to keep our farmers from overstering home mortgages deregulated--and to prevent americans putting more of the house on the backs of loans. they have the same strategy the federal government is just pushing forward: depreciate mortgages, savers, corporate americans cut to rock bottom mortgages for savers who don't know what's going on, government cannot keep down a financial recession. this year, our economy in succor, anything without appreciation for the commodity prices, did not have all the same characteristics six years ago. pieces of our common stock were already exhausted. the good things of life depend on the leadership and the prospect of enduring life. our only hope left alive is that, once compassion has been given, compassion will breed for both ourselves and the children of our grandparents.\n",
      "to gain those results, I urge congress to expand medicare coverage for every eligible AFSC, and to improve medicaid services for the elderly. again, i recommend a bipartisan increase in medicare revenue for\n"
     ]
    }
   ],
   "source": [
    "text = \"I will explain the meaning of life. The meaning of life is\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    res = model.generate(input_ids, ntokens=300)\n",
    "    output = tokenizer.decode(res[0].tolist())\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will explain the meaning of life. The meaning of life is to serve it by serving freedom. and life in a free country is nothing but gratitude for a country which has serviced us in spite of the pressures of austerity and unemployment.\n",
      "well, yes. by the time too are we ready. so let's back to the business of agriculture. as i said, the livestock emissions caps have been cut--a's focus. all of us at home today will agree. we must take action now to keep our farmers from overstering home mortgages deregulated--and to prevent americans putting more of the house on the backs of loans. they have the same strategy the federal government is just pushing forward: depreciate mortgages, savers, corporate americans cut to rock bottom mortgages for savers who don't know what's going on, government cannot keep down a financial recession. this year, our economy in succor, anything without appreciation for the commodity prices, did not have all the same characteristics six years ago. pieces of our common stock were already exhausted. the good things of life depend on the leadership and the prospect of enduring life. our only hope left alive is that, once compassion has been given, compassion will breed for both ourselves and the children of our grandparents.\n",
      "to gain those results, I urge congress to expand medicare coverage for every eligible AFSC, and to improve medicaid services for the elderly. again, i recommend a bipartisan increase in medicare revenue for\n"
     ]
    }
   ],
   "source": [
    "text = \"I will explain the meaning of life. The meaning of life is\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    res = model.generate(input_ids, ntokens=300)\n",
    "    output = tokenizer.decode(res[0].tolist())\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will explain the meaning of life. The meaning of life is to serve it by serving freedom. and life in a free country is nothing but gratitude for a\n",
      "-----------\n",
      " country which has serviced us in spite of the pressures of austerity and unemployment.\n",
      "well, yes. by the time too are we ready. so let's\n",
      "-----------\n",
      " back to the business of agriculture. as i said, the livestock emissions caps have been cut--a's focus. all of us at home today will agree.\n",
      "-----------\n",
      " we must take action now to keep our farmers from overstering home mortgages deregulated--and to prevent americans putting more of the house on the backs of\n",
      "-----------\n",
      " loans. they have the same strategy the federal government is just pushing forward: depreciate mortgages, savers, corporate americans cut to rock bottom mortgages for\n",
      "-----------\n",
      " savers who don't know what's going on, government cannot keep down a financial recession. this year, our economy in succor, anything without appreciation for\n",
      "-----------\n",
      " the commodity prices, did not have all the same characteristics six years ago. pieces of our common stock were already exhausted. the good things of life depend on the\n",
      "-----------\n",
      " leadership and the prospect of enduring life. our only hope left alive is that, once compassion has been given, compassion will breed for both ourselves and the children of\n",
      "-----------\n",
      " our grandparents.\n",
      "to gain those results, I urge congress to expand medicare coverage for every eligible AFSC, and to improve medicaid services for the elderly\n",
      "-----------\n",
      ". again, i recommend a bipartisan increase in medicare revenue for\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "size = 32\n",
    "while i*size < len(res[0]):\n",
    "    print(tokenizer.decode(res[0][i*size:size+i*size].tolist()))\n",
    "    print('-----------')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will explain the meaning of life. The meaning of life is that we are living in a world that is not ours. We are living in a world that is not ours. We are living in a world that is not ours. We are living in a world that is not ours. We are living in a world that is not ours. We are living in a world that is not ours. We are living in a world that is not ours. We are living in a world that is not\n"
     ]
    }
   ],
   "source": [
    "model2 = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "res = model2.generate(input_ids, max_length=100)\n",
    "output = tokenizer.decode(res[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 2, Average Loss: 4.411460208892822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.411460208892822"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch: 1, Step: 0, loss_token: 3.2408559322357178\n",
    "Epoch: 1, Step: 1, loss_token: 3.53310227394104\n",
    "Epoch: 1, Step: 2, loss_token: 3.226858615875244\n",
    "Epoch: 1, Step: 3, loss_token: 3.383723258972168\n",
    "Epoch: 1, Step: 4, loss_token: 3.362529754638672\n",
    "Epoch: 1, Step: 5, loss_token: 3.258737325668335\n",
    "Epoch: 1, Step: 6, loss_token: 3.2521677017211914\n",
    "Epoch: 1, Step: 7, loss_token: 3.2775447368621826\n",
    "Epoch: 1, Step: 8, loss_token: 3.3377628326416016\n",
    "Epoch: 1, Step: 9, loss_token: 3.27563214302063\n",
    "Epoch: 1, Average Loss: 0.001695163107930288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch: 1, Step: 0, loss_token: 3.429766893386841\n",
    "Epoch: 1, Step: 1, loss_token: 3.404374122619629\n",
    "Epoch: 1, Step: 2, loss_token: 3.586282968521118\n",
    "Epoch: 1, Step: 3, loss_token: 3.647005796432495\n",
    "Epoch: 1, Step: 4, loss_token: 3.5233004093170166\n",
    "Epoch: 1, Step: 5, loss_token: 3.4722964763641357\n",
    "Epoch: 1, Step: 6, loss_token: 3.5063366889953613\n",
    "Epoch: 1, Step: 7, loss_token: 3.5912227630615234\n",
    "Epoch: 1, Step: 8, loss_token: 3.6837527751922607\n",
    "Epoch: 1, Step: 9, loss_token: 3.5423386096954346\n",
    "Epoch: 1, Average Loss: 0.0018095974177236417"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # A lightweight model for embeddings\n",
    "\n",
    "def get_embedding(text):\n",
    "    return model.encode(text, convert_to_tensor=True)  # Returns a tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8691, device='cuda:0')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is an example sentence.\"\n",
    "text2 = \"that sentence is an example\"\n",
    "embedding = get_embedding(text)\n",
    "embedding2 = get_embedding(text2)\n",
    "\n",
    "torch.sum(embedding * embedding2) / (torch.norm(embedding) * torch.norm(embedding2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a new tokenizer from an old one",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdval7tUZwdZ"
   },
   "source": [
    "# Groupe Relative Policy Optimization (GRPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CorBhMaiZwdb"
   },
   "source": [
    "Install the Hugging Face libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, level, predictor=None, model_name = \"gpt2\"):\n",
    "        super(Embedder, self).__init__()\n",
    "        if level ==1:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_name).get_input_embeddings().to(device)\n",
    "            self.model.requires_grad_(False)\n",
    "        else:\n",
    "            if predictor is None:\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(model_name).transformer.to(device)\n",
    "                self.model.wte = nn.Identity()\n",
    "            else:\n",
    "                self.model = predictor\n",
    "        self.level = level\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, 768))\n",
    "        self.cls_token.requires_grad_(True)\n",
    "        self.no_pred = predictor is None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.level == 1:\n",
    "            return self.model(input.squeeze(-1))\n",
    "        cls = self.cls_token.repeat(input.size(0), 1, 1)\n",
    "        input = torch.cat([input, cls], dim=1)\n",
    "        if self.no_pred:\n",
    "            out = self.model(inputs_embeds = input)[0][:, -1, :]\n",
    "        else:\n",
    "            out = self.model(input)[:,-1]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Abstract_level(nn.Module):\n",
    "    def __init__(self, level, model_name = \"gpt2\"):\n",
    "        super(Abstract_level, self).__init__()\n",
    "        self.abstract_size = level\n",
    "\n",
    "        self.level = level\n",
    "        if level==1:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_name).transformer.to(device)\n",
    "            self.model.wte = nn.Identity()\n",
    "            self.model.requires_grad_(False)\n",
    "        else:\n",
    "            config = AutoConfig.from_pretrained(model_name)\n",
    "            # Create the model from configuration (with random weights)\n",
    "            self.model = AutoModelForCausalLM.from_config(config).transformer.to(device)\n",
    "            self.model.wte = nn.Identity()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        out = self.model(inputs_embeds = input)[0]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token_pred(nn.Module):\n",
    "    def __init__(self, model_name = \"gpt2\"):\n",
    "        super(Token_pred, self).__init__()\n",
    "        self.layer = AutoModelForCausalLM.from_pretrained(model_name).lm_head.to(device)\n",
    "        self.layer.requires_grad_(False)\n",
    "        #self.layer = nn.Linear(768*2, 50257).to(device)\n",
    "        #self.lin2 = nn.Linear(768, 768).to(device) n\n",
    "    \n",
    "    def forward(self, input):\n",
    "        #x = torch.cat([input, condition], dim = -1)\n",
    "        #x = input + condition\n",
    "        return self.layer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldModel_LLM(nn.Module):\n",
    "    def __init__(self, level, use_abstract = True, sum_inside=False):\n",
    "        super(WorldModel_LLM, self).__init__()\n",
    "        self.alevel = level\n",
    "        self.aalevel = 4\n",
    "        self.use_abstract = use_abstract\n",
    "\n",
    "        self.lvl0_predictor = Abstract_level(1)\n",
    "        if use_abstract:\n",
    "            self.lvl1_predictor = Abstract_level(self.alevel)\n",
    "\n",
    "        self.token_embedder = Embedder(1)\n",
    "        if use_abstract:\n",
    "            self.lvl1_embedder = Embedder(self.alevel)\n",
    "            #self.lvl1_embedder = Embedder(self.alevel, predictor=self.lvl0_predictor)\n",
    "\n",
    "        self.token_pred = Token_pred()\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.lvl1_embed_save = []\n",
    "        self.lvl1_embed_pred_save = [torch.zeros(768).to(device)]\n",
    "\n",
    "        self.sum_inside = sum_inside\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        nseq = tokens.shape[1]//self.alevel\n",
    "        self.aalevel = nseq\n",
    "        tokens = tokens[:, :tokens.shape[1]//self.alevel*self.alevel]\n",
    "        \n",
    "        tokens = tokens.reshape(tokens.shape[0], -1, self.alevel).reshape(-1, self.alevel) # (batch_size * nseq, alevel)\n",
    "        token_embedding = self.token_embedder(tokens) # (batch_size * nseq, alevel, embedding_size)\n",
    "\n",
    "        if self.use_abstract:\n",
    "            lvl1_embedding = self.lvl1_embedder(token_embedding).reshape(-1, nseq, 768) # (batch_size, nseq, embedding_size)\n",
    "        \n",
    "            lvl1_embedding_pred = self.lvl1_predictor(lvl1_embedding) # (batch_size, nseq, embedding_size)\n",
    "            lvl1_embedding_pred = torch.cat((torch.zeros_like(lvl1_embedding_pred[:,:1]),lvl1_embedding_pred[:,:-1]), dim = 1)\n",
    "            lvl1_embedding_pred = lvl1_embedding_pred.unsqueeze(2).repeat(1,1, self.alevel, 1).reshape(-1, self.alevel, 768) # (batch_size * nseq, alevel, embedding_size)\n",
    "\n",
    "            if self.sum_inside:\n",
    "                token_embedding_pred = self.lvl0_predictor(token_embedding[:,:-1] + lvl1_embedding_pred[:,:-1]) # (batch_size * nseq, alevel, embedding_size)\n",
    "            else:\n",
    "                token_embedding_pred = self.lvl0_predictor(token_embedding[:,:-1])+ lvl1_embedding_pred[:,:-1] # (batch_size * nseq, alevel, embedding_size)\n",
    "            tokens_pred = self.token_pred(token_embedding_pred) # (batch_size * nseq, alevel, vocab_size)\n",
    "        else:\n",
    "            token_embedding_pred = self.lvl0_predictor(token_embedding[:,:-1]) # (batch_size * nseq, alevel, embedding_size)\n",
    "            tokens_pred = self.token_pred(token_embedding_pred) # (batch_size * nseq, alevel, vocab_size)\n",
    "\n",
    "        loss_token = self.criterion(tokens_pred.reshape(-1, 50257), tokens[:,1:].reshape(-1))\n",
    "        return tokens_pred, loss_token\n",
    "    \n",
    "    def generate(self, tokens, ntokens):\n",
    "        with torch.no_grad():\n",
    "            while tokens.shape[1] <= ntokens:\n",
    "                if tokens.shape[1] < self.alevel:\n",
    "                    token_embedding = self.token_embedder(tokens)\n",
    "                    token_embedding_pred = self.lvl0_predictor(token_embedding)[:,-1]\n",
    "\n",
    "                else:\n",
    "                    reste = tokens.shape[1]%self.alevel\n",
    "                    token_embedding = self.token_embedder(tokens[:, -self.alevel:] )\n",
    "\n",
    "                    if reste==0:\n",
    "                        new_embedding = self.lvl1_embedder(token_embedding)[-1]\n",
    "                        self.lvl1_embed_save.append(new_embedding)\n",
    "                        lvl1_embedding= torch.stack(self.lvl1_embed_save)\n",
    "                        self.lvl1_embed_pred_save.append(self.lvl1_predictor(lvl1_embedding[-self.aalevel:])[-1])\n",
    "                        lvl1_embed_pred_save = torch.stack(self.lvl1_embed_pred_save)\n",
    "                    \n",
    "                        lvl1_embedding_pred = lvl1_embed_pred_save[-2].repeat(self.alevel-reste, 1)\n",
    "                    else:\n",
    "                        current_pred_lvl1 = lvl1_embed_pred_save[-1].repeat(reste, 1)\n",
    "                        prev_pred_lvl1 = lvl1_embed_pred_save[-2].repeat(self.alevel-reste, 1)\n",
    "                        lvl1_embedding_pred = torch.cat([prev_pred_lvl1, current_pred_lvl1], dim = 0)\n",
    "                    \n",
    "                    if self.sum_inside:\n",
    "                        token_embedding_pred = self.lvl0_predictor(token_embedding[:,1:] + lvl1_embedding_pred[1:])[:,-1]\n",
    "                    else:\n",
    "                        token_embedding_pred = self.lvl0_predictor(token_embedding[:,1:])[:,-1] + lvl1_embedding_pred[1:][-1]\n",
    "\n",
    "                tokens_pred = self.token_pred(token_embedding_pred)\n",
    "                next_token = torch.multinomial(tokens_pred.softmax(-1), 1)\n",
    "                tokens = torch.cat([tokens, next_token], dim = 1)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "This is an example of me and of Starting:\n",
      "k.\n",
      "\n",
      "\n",
      "s in to on,\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "1:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an example of me and of\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "print(input_ids.shape)\n",
    "\n",
    "wmllm = WorldModel_LLM(2).to(device)\n",
    "wmllm(input_ids)\n",
    "with torch.no_grad():\n",
    "    #op = wmllm(input_ids)\n",
    "    op = wmllm.generate(input_ids, 32)\n",
    "    print(tokenizer.decode(op[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmllm.aalevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr. speaker, mr. president, and distinguished members of the house and senate, honored guests, and fellow citizens: less than 3 weeks ago, i joined you on the west front of this very building and, looking over the monuments to our proud past, offered you my hand in filling the next page of american history with a story of extended prosperity and continued peace. and tonight i'm back to offer you my plans as well. the hand remains extended; the sleeves are rolled up; america is waiting; and now we must produce. together, we can build a better america.\n",
      "it is comforting to return to this historic chamber. here, 22 years ago, i first raised my hand to be sworn into public life. so, tonight i feel as if i'm returning home to friends. and i intend, in the months and years to come, to give you what friends deserve: frankness, respect, and my best judgment about ways to improve america's future. in return, i ask for an honest commitment to our common mission of progress. if we seize the opport\n"
     ]
    }
   ],
   "source": [
    "with open('sotu.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('  ', '\\n')\n",
    "\n",
    "\n",
    "text = text.lower()\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (260738 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([260738])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tokenizer.encode(text, return_tensors='pt').squeeze()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([260738]),\n",
       " tensor([43395,    13, 10834,    11,   285,    81,    13,  1893,    11,   290,\n",
       "         18876,  1866,   286,   262,  2156,   290, 34548,    11, 21014, 10650,\n",
       "            11,   290,  5891,  4290,    25,  1342,   621,   513,  2745,  2084,\n",
       "            11,  1312,  5399,   345,   319,   262,  7421,  2166,   286,   428,\n",
       "           845,  2615,   290,    11,  2045,   625,   262, 28814,   284,   674]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "lendata = train_data.shape[0]\n",
    "data.shape, data[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 128\n",
    "batch_size = 32\n",
    " \n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_length - 1, (batch_size,))\n",
    "    X = torch.stack([data[i:i+context_length] for i in ix])\n",
    "    return X\n",
    "\n",
    "X = get_batch(\"train\")\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  validation(model):\n",
    "    # Define hyperparameters\n",
    "    epoch = 1\n",
    "\n",
    "    # Training loop\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        num =5\n",
    "        for i in range(num):\n",
    "            # Get batch\n",
    "            X = get_batch('val')\n",
    "            \n",
    "            # Forward pass\n",
    "            output, loss_token = model(X.to(device))\n",
    "            loss = loss_token\n",
    "            \n",
    "            total_loss += loss.item()  \n",
    "\n",
    "        avg_loss = total_loss / num \n",
    "        print(f\"Validation: {epoch+1}, Average Loss: {     avg_loss}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WorldModel_LLM(16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#model.load_state_dict(torch.load(\"MW_sum_inside.pt\"))\n",
    "best_loss = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "Epoch: 1, Step: 0, loss_token: 5.647838115692139\n",
      "Epoch: 1, Step: 1, loss_token: 6.901641845703125\n",
      "Epoch: 1, Step: 2, loss_token: 5.1891279220581055\n",
      "Epoch: 1, Step: 3, loss_token: 5.018613338470459\n",
      "Epoch: 1, Step: 4, loss_token: 4.989016056060791\n",
      "Epoch: 1, Step: 5, loss_token: 4.783699035644531\n",
      "Epoch: 1, Step: 6, loss_token: 4.966264247894287\n",
      "Epoch: 1, Step: 7, loss_token: 4.771054744720459\n",
      "Epoch: 1, Step: 8, loss_token: 4.942931652069092\n",
      "Epoch: 1, Step: 9, loss_token: 4.854284763336182\n",
      "Epoch: 1, Step: 10, loss_token: 4.758890628814697\n",
      "Epoch: 1, Step: 11, loss_token: 4.736922264099121\n",
      "Epoch: 1, Step: 12, loss_token: 4.703001499176025\n",
      "Epoch: 1, Step: 13, loss_token: 4.7543768882751465\n",
      "Epoch: 1, Step: 14, loss_token: 4.678609848022461\n",
      "Epoch: 1, Step: 15, loss_token: 4.812869548797607\n",
      "Epoch: 1, Step: 16, loss_token: 4.659309387207031\n",
      "Epoch: 1, Step: 17, loss_token: 4.771054267883301\n",
      "Epoch: 1, Step: 18, loss_token: 4.656765937805176\n",
      "Epoch: 1, Step: 19, loss_token: 4.583284378051758\n",
      "Epoch: 1, Step: 20, loss_token: 4.682955741882324\n",
      "Epoch: 1, Step: 21, loss_token: 4.692956924438477\n",
      "Epoch: 1, Step: 22, loss_token: 4.638615608215332\n",
      "Epoch: 1, Step: 23, loss_token: 4.622591972351074\n",
      "Epoch: 1, Step: 24, loss_token: 4.5448899269104\n",
      "Epoch: 1, Step: 25, loss_token: 4.625372409820557\n",
      "Epoch: 1, Step: 26, loss_token: 4.578306674957275\n",
      "Epoch: 1, Step: 27, loss_token: 4.580648422241211\n",
      "Epoch: 1, Step: 28, loss_token: 4.545079231262207\n",
      "Epoch: 1, Step: 29, loss_token: 4.663531303405762\n",
      "Epoch: 1, Step: 30, loss_token: 4.4760847091674805\n",
      "Epoch: 1, Step: 31, loss_token: 4.705488204956055\n",
      "Epoch: 1, Step: 32, loss_token: 4.696284770965576\n",
      "Epoch: 1, Step: 33, loss_token: 4.488278865814209\n",
      "Epoch: 1, Step: 34, loss_token: 4.615029335021973\n",
      "Epoch: 1, Step: 35, loss_token: 4.556642532348633\n",
      "Epoch: 1, Step: 36, loss_token: 4.663107872009277\n",
      "Epoch: 1, Step: 37, loss_token: 4.666800498962402\n",
      "Epoch: 1, Step: 38, loss_token: 4.597190856933594\n",
      "Epoch: 1, Step: 39, loss_token: 4.682003498077393\n",
      "Epoch: 1, Step: 40, loss_token: 4.648469924926758\n",
      "Epoch: 1, Step: 41, loss_token: 4.614583969116211\n",
      "Epoch: 1, Step: 42, loss_token: 4.577993869781494\n",
      "Epoch: 1, Step: 43, loss_token: 4.589879035949707\n",
      "Epoch: 1, Step: 44, loss_token: 4.616507053375244\n",
      "Epoch: 1, Step: 45, loss_token: 4.610677242279053\n",
      "Epoch: 1, Step: 46, loss_token: 4.489865779876709\n",
      "Epoch: 1, Step: 47, loss_token: 4.501338005065918\n",
      "Epoch: 1, Step: 48, loss_token: 4.625174522399902\n",
      "Epoch: 1, Step: 49, loss_token: 4.607276439666748\n",
      "Epoch: 1, Step: 50, loss_token: 4.540122032165527\n",
      "Epoch: 1, Step: 51, loss_token: 4.55612325668335\n",
      "Epoch: 1, Step: 52, loss_token: 4.455995082855225\n",
      "Epoch: 1, Step: 53, loss_token: 4.511666297912598\n",
      "Epoch: 1, Step: 54, loss_token: 4.424663066864014\n",
      "Epoch: 1, Step: 55, loss_token: 4.50976037979126\n",
      "Epoch: 1, Step: 56, loss_token: 4.48428201675415\n",
      "Epoch: 1, Step: 57, loss_token: 4.579199314117432\n",
      "Epoch: 1, Step: 58, loss_token: 4.489701747894287\n",
      "Epoch: 1, Step: 59, loss_token: 4.62003231048584\n",
      "Epoch: 1, Step: 60, loss_token: 4.475240230560303\n",
      "Epoch: 1, Step: 61, loss_token: 4.457022190093994\n",
      "Epoch: 1, Step: 62, loss_token: 4.454930782318115\n",
      "Epoch: 1, Step: 63, loss_token: 4.487324237823486\n",
      "Epoch: 1, Step: 64, loss_token: 4.556213855743408\n",
      "Epoch: 1, Step: 65, loss_token: 4.54728889465332\n",
      "Epoch: 1, Step: 66, loss_token: 4.45757532119751\n",
      "Epoch: 1, Step: 67, loss_token: 4.58073616027832\n",
      "Epoch: 1, Step: 68, loss_token: 4.429932117462158\n",
      "Epoch: 1, Step: 69, loss_token: 4.497818470001221\n",
      "Epoch: 1, Step: 70, loss_token: 4.492921352386475\n",
      "Epoch: 1, Step: 71, loss_token: 4.520271301269531\n",
      "Epoch: 1, Step: 72, loss_token: 4.5504469871521\n",
      "Epoch: 1, Step: 73, loss_token: 4.6557393074035645\n",
      "Epoch: 1, Step: 74, loss_token: 4.5159101486206055\n",
      "Epoch: 1, Step: 75, loss_token: 4.509893894195557\n",
      "Epoch: 1, Average Loss: 4.667684109587419\n",
      "Validation: 2, Average Loss: 4.316109085083008\n",
      "Model saved\n",
      "Epoch: 2, Step: 0, loss_token: 4.558615207672119\n",
      "Epoch: 2, Step: 1, loss_token: 4.556772708892822\n",
      "Epoch: 2, Step: 2, loss_token: 4.494325160980225\n",
      "Epoch: 2, Step: 3, loss_token: 4.591951847076416\n",
      "Epoch: 2, Step: 4, loss_token: 4.498382091522217\n",
      "Epoch: 2, Step: 5, loss_token: 4.518468856811523\n",
      "Epoch: 2, Step: 6, loss_token: 4.449991703033447\n",
      "Epoch: 2, Step: 7, loss_token: 4.490080833435059\n",
      "Epoch: 2, Step: 8, loss_token: 4.562142372131348\n",
      "Epoch: 2, Step: 9, loss_token: 4.5188517570495605\n",
      "Epoch: 2, Step: 10, loss_token: 4.476025581359863\n",
      "Epoch: 2, Step: 11, loss_token: 4.4598517417907715\n",
      "Epoch: 2, Step: 12, loss_token: 4.4890007972717285\n",
      "Epoch: 2, Step: 13, loss_token: 4.419833183288574\n",
      "Epoch: 2, Step: 14, loss_token: 4.477572441101074\n",
      "Epoch: 2, Step: 15, loss_token: 4.570023536682129\n",
      "Epoch: 2, Step: 16, loss_token: 4.5050811767578125\n",
      "Epoch: 2, Step: 17, loss_token: 4.507535934448242\n",
      "Epoch: 2, Step: 18, loss_token: 4.432919025421143\n",
      "Epoch: 2, Step: 19, loss_token: 4.4413299560546875\n",
      "Epoch: 2, Step: 20, loss_token: 4.55204439163208\n",
      "Epoch: 2, Step: 21, loss_token: 4.505601406097412\n",
      "Epoch: 2, Step: 22, loss_token: 4.406353950500488\n",
      "Epoch: 2, Step: 23, loss_token: 4.4565510749816895\n",
      "Epoch: 2, Step: 24, loss_token: 4.549530982971191\n",
      "Epoch: 2, Step: 25, loss_token: 4.413877010345459\n",
      "Epoch: 2, Step: 26, loss_token: 4.559765815734863\n",
      "Epoch: 2, Step: 27, loss_token: 4.401692867279053\n",
      "Epoch: 2, Step: 28, loss_token: 4.489885330200195\n",
      "Epoch: 2, Step: 29, loss_token: 4.563304424285889\n",
      "Epoch: 2, Step: 30, loss_token: 4.58580207824707\n",
      "Epoch: 2, Step: 31, loss_token: 4.339812278747559\n",
      "Epoch: 2, Step: 32, loss_token: 4.497001647949219\n",
      "Epoch: 2, Step: 33, loss_token: 4.4305877685546875\n",
      "Epoch: 2, Step: 34, loss_token: 4.510939598083496\n",
      "Epoch: 2, Step: 35, loss_token: 4.547428607940674\n",
      "Epoch: 2, Step: 36, loss_token: 4.38193941116333\n",
      "Epoch: 2, Step: 37, loss_token: 4.473386764526367\n",
      "Epoch: 2, Step: 38, loss_token: 4.535372257232666\n",
      "Epoch: 2, Step: 39, loss_token: 4.627286911010742\n",
      "Epoch: 2, Step: 40, loss_token: 4.4499359130859375\n",
      "Epoch: 2, Step: 41, loss_token: 4.59724235534668\n",
      "Epoch: 2, Step: 42, loss_token: 4.430658340454102\n",
      "Epoch: 2, Step: 43, loss_token: 4.449166297912598\n",
      "Epoch: 2, Step: 44, loss_token: 4.475796222686768\n",
      "Epoch: 2, Step: 45, loss_token: 4.498231887817383\n",
      "Epoch: 2, Step: 46, loss_token: 4.38710355758667\n",
      "Epoch: 2, Step: 47, loss_token: 4.3942036628723145\n",
      "Epoch: 2, Step: 48, loss_token: 4.414817810058594\n",
      "Epoch: 2, Step: 49, loss_token: 4.450197219848633\n",
      "Epoch: 2, Step: 50, loss_token: 4.4113240242004395\n",
      "Epoch: 2, Step: 51, loss_token: 4.5390706062316895\n",
      "Epoch: 2, Step: 52, loss_token: 4.508475303649902\n",
      "Epoch: 2, Step: 53, loss_token: 4.510480880737305\n",
      "Epoch: 2, Step: 54, loss_token: 4.479544162750244\n",
      "Epoch: 2, Step: 55, loss_token: 4.407598972320557\n",
      "Epoch: 2, Step: 56, loss_token: 4.480659008026123\n",
      "Epoch: 2, Step: 57, loss_token: 4.445292949676514\n",
      "Epoch: 2, Step: 58, loss_token: 4.51012659072876\n",
      "Epoch: 2, Step: 59, loss_token: 4.580558776855469\n",
      "Epoch: 2, Step: 60, loss_token: 4.420689105987549\n",
      "Epoch: 2, Step: 61, loss_token: 4.392496109008789\n",
      "Epoch: 2, Step: 62, loss_token: 4.428285121917725\n",
      "Epoch: 2, Step: 63, loss_token: 4.423068046569824\n",
      "Epoch: 2, Step: 64, loss_token: 4.369697093963623\n",
      "Epoch: 2, Step: 65, loss_token: 4.443911552429199\n",
      "Epoch: 2, Step: 66, loss_token: 4.4770426750183105\n",
      "Epoch: 2, Step: 67, loss_token: 4.386609077453613\n",
      "Epoch: 2, Step: 68, loss_token: 4.412359237670898\n",
      "Epoch: 2, Step: 69, loss_token: 4.414833068847656\n",
      "Epoch: 2, Step: 70, loss_token: 4.510092735290527\n",
      "Epoch: 2, Step: 71, loss_token: 4.48996639251709\n",
      "Epoch: 2, Step: 72, loss_token: 4.508608341217041\n",
      "Epoch: 2, Step: 73, loss_token: 4.385000228881836\n",
      "Epoch: 2, Step: 74, loss_token: 4.329371929168701\n",
      "Epoch: 2, Step: 75, loss_token: 4.470705509185791\n",
      "Epoch: 2, Average Loss: 4.475396595503154\n",
      "Validation: 2, Average Loss: 4.293203353881836\n",
      "Model saved\n",
      "Epoch: 3, Step: 0, loss_token: 4.518407344818115\n",
      "Epoch: 3, Step: 1, loss_token: 4.433944225311279\n",
      "Epoch: 3, Step: 2, loss_token: 4.419117450714111\n",
      "Epoch: 3, Step: 3, loss_token: 4.498838424682617\n",
      "Epoch: 3, Step: 4, loss_token: 4.4868974685668945\n",
      "Epoch: 3, Step: 5, loss_token: 4.524648666381836\n",
      "Epoch: 3, Step: 6, loss_token: 4.438417434692383\n",
      "Epoch: 3, Step: 7, loss_token: 4.467672824859619\n",
      "Epoch: 3, Step: 8, loss_token: 4.418083190917969\n",
      "Epoch: 3, Step: 9, loss_token: 4.349000930786133\n",
      "Epoch: 3, Step: 10, loss_token: 4.421114921569824\n",
      "Epoch: 3, Step: 11, loss_token: 4.525204181671143\n",
      "Epoch: 3, Step: 12, loss_token: 4.447967052459717\n",
      "Epoch: 3, Step: 13, loss_token: 4.500466823577881\n",
      "Epoch: 3, Step: 14, loss_token: 4.412130355834961\n",
      "Epoch: 3, Step: 15, loss_token: 4.460850715637207\n",
      "Epoch: 3, Step: 16, loss_token: 4.512291431427002\n",
      "Epoch: 3, Step: 17, loss_token: 4.435831546783447\n",
      "Epoch: 3, Step: 18, loss_token: 4.432134628295898\n",
      "Epoch: 3, Step: 19, loss_token: 4.4117608070373535\n",
      "Epoch: 3, Step: 20, loss_token: 4.3845696449279785\n",
      "Epoch: 3, Step: 21, loss_token: 4.385952472686768\n",
      "Epoch: 3, Step: 22, loss_token: 4.4356818199157715\n",
      "Epoch: 3, Step: 23, loss_token: 4.392199993133545\n",
      "Epoch: 3, Step: 24, loss_token: 4.516744613647461\n",
      "Epoch: 3, Step: 25, loss_token: 4.489703178405762\n",
      "Epoch: 3, Step: 26, loss_token: 4.341374397277832\n",
      "Epoch: 3, Step: 27, loss_token: 4.503159999847412\n",
      "Epoch: 3, Step: 28, loss_token: 4.487696647644043\n",
      "Epoch: 3, Step: 29, loss_token: 4.405359745025635\n",
      "Epoch: 3, Step: 30, loss_token: 4.55861234664917\n",
      "Epoch: 3, Step: 31, loss_token: 4.493820667266846\n",
      "Epoch: 3, Step: 32, loss_token: 4.343537330627441\n",
      "Epoch: 3, Step: 33, loss_token: 4.481209754943848\n",
      "Epoch: 3, Step: 34, loss_token: 4.464629650115967\n",
      "Epoch: 3, Step: 35, loss_token: 4.44955587387085\n",
      "Epoch: 3, Step: 36, loss_token: 4.317466735839844\n",
      "Epoch: 3, Step: 37, loss_token: 4.258033275604248\n",
      "Epoch: 3, Step: 38, loss_token: 4.4625396728515625\n",
      "Epoch: 3, Step: 39, loss_token: 4.368203639984131\n",
      "Epoch: 3, Step: 40, loss_token: 4.476874828338623\n",
      "Epoch: 3, Step: 41, loss_token: 4.406769275665283\n",
      "Epoch: 3, Step: 42, loss_token: 4.457028865814209\n",
      "Epoch: 3, Step: 43, loss_token: 4.404685974121094\n",
      "Epoch: 3, Step: 44, loss_token: 4.506059646606445\n",
      "Epoch: 3, Step: 45, loss_token: 4.494801998138428\n",
      "Epoch: 3, Step: 46, loss_token: 4.487823009490967\n",
      "Epoch: 3, Step: 47, loss_token: 4.421225547790527\n",
      "Epoch: 3, Step: 48, loss_token: 4.418245315551758\n",
      "Epoch: 3, Step: 49, loss_token: 4.440446376800537\n",
      "Epoch: 3, Step: 50, loss_token: 4.381284236907959\n",
      "Epoch: 3, Step: 51, loss_token: 4.43325662612915\n",
      "Epoch: 3, Step: 52, loss_token: 4.428406238555908\n",
      "Epoch: 3, Step: 53, loss_token: 4.357272624969482\n",
      "Epoch: 3, Step: 54, loss_token: 4.316930770874023\n",
      "Epoch: 3, Step: 55, loss_token: 4.415403366088867\n",
      "Epoch: 3, Step: 56, loss_token: 4.3768310546875\n",
      "Epoch: 3, Step: 57, loss_token: 4.4958038330078125\n",
      "Epoch: 3, Step: 58, loss_token: 4.441267013549805\n",
      "Epoch: 3, Step: 59, loss_token: 4.5392913818359375\n",
      "Epoch: 3, Step: 60, loss_token: 4.3886213302612305\n",
      "Epoch: 3, Step: 61, loss_token: 4.503553867340088\n",
      "Epoch: 3, Step: 62, loss_token: 4.467619895935059\n",
      "Epoch: 3, Step: 63, loss_token: 4.442916393280029\n",
      "Epoch: 3, Step: 64, loss_token: 4.5135955810546875\n",
      "Epoch: 3, Step: 65, loss_token: 4.392031192779541\n",
      "Epoch: 3, Step: 66, loss_token: 4.443193435668945\n",
      "Epoch: 3, Step: 67, loss_token: 4.542283058166504\n",
      "Epoch: 3, Step: 68, loss_token: 4.421579837799072\n",
      "Epoch: 3, Step: 69, loss_token: 4.355213642120361\n",
      "Epoch: 3, Step: 70, loss_token: 4.421244144439697\n",
      "Epoch: 3, Step: 71, loss_token: 4.361839771270752\n",
      "Epoch: 3, Step: 72, loss_token: 4.3506178855896\n",
      "Epoch: 3, Step: 73, loss_token: 4.558218955993652\n",
      "Epoch: 3, Step: 74, loss_token: 4.383270263671875\n",
      "Epoch: 3, Step: 75, loss_token: 4.52083683013916\n",
      "Epoch: 3, Average Loss: 4.43975231522008\n",
      "Validation: 2, Average Loss: 4.247534275054932\n",
      "Model saved\n",
      "Epoch: 4, Step: 0, loss_token: 4.374660015106201\n",
      "Epoch: 4, Step: 1, loss_token: 4.39980411529541\n",
      "Epoch: 4, Step: 2, loss_token: 4.428770542144775\n",
      "Epoch: 4, Step: 3, loss_token: 4.4894537925720215\n",
      "Epoch: 4, Step: 4, loss_token: 4.405517101287842\n",
      "Epoch: 4, Step: 5, loss_token: 4.362620830535889\n",
      "Epoch: 4, Step: 6, loss_token: 4.514589786529541\n",
      "Epoch: 4, Step: 7, loss_token: 4.427675724029541\n",
      "Epoch: 4, Step: 8, loss_token: 4.524977207183838\n",
      "Epoch: 4, Step: 9, loss_token: 4.375926494598389\n",
      "Epoch: 4, Step: 10, loss_token: 4.378416538238525\n",
      "Epoch: 4, Step: 11, loss_token: 4.438195705413818\n",
      "Epoch: 4, Step: 12, loss_token: 4.415494441986084\n",
      "Epoch: 4, Step: 13, loss_token: 4.270964622497559\n",
      "Epoch: 4, Step: 14, loss_token: 4.3473005294799805\n",
      "Epoch: 4, Step: 15, loss_token: 4.398718357086182\n",
      "Epoch: 4, Step: 16, loss_token: 4.423275470733643\n",
      "Epoch: 4, Step: 17, loss_token: 4.379632949829102\n",
      "Epoch: 4, Step: 18, loss_token: 4.441258430480957\n",
      "Epoch: 4, Step: 19, loss_token: 4.437868595123291\n",
      "Epoch: 4, Step: 20, loss_token: 4.35491943359375\n",
      "Epoch: 4, Step: 21, loss_token: 4.333302021026611\n",
      "Epoch: 4, Step: 22, loss_token: 4.408559322357178\n",
      "Epoch: 4, Step: 23, loss_token: 4.4472150802612305\n",
      "Epoch: 4, Step: 24, loss_token: 4.355190753936768\n",
      "Epoch: 4, Step: 25, loss_token: 4.282532215118408\n",
      "Epoch: 4, Step: 26, loss_token: 4.37779426574707\n",
      "Epoch: 4, Step: 27, loss_token: 4.4127583503723145\n",
      "Epoch: 4, Step: 28, loss_token: 4.402841091156006\n",
      "Epoch: 4, Step: 29, loss_token: 4.455554485321045\n",
      "Epoch: 4, Step: 30, loss_token: 4.494820594787598\n",
      "Epoch: 4, Step: 31, loss_token: 4.461454391479492\n",
      "Epoch: 4, Step: 32, loss_token: 4.30872917175293\n",
      "Epoch: 4, Step: 33, loss_token: 4.414161205291748\n",
      "Epoch: 4, Step: 34, loss_token: 4.379327297210693\n",
      "Epoch: 4, Step: 35, loss_token: 4.469268798828125\n",
      "Epoch: 4, Step: 36, loss_token: 4.36329460144043\n",
      "Epoch: 4, Step: 37, loss_token: 4.297323226928711\n",
      "Epoch: 4, Step: 38, loss_token: 4.546839714050293\n",
      "Epoch: 4, Step: 39, loss_token: 4.477288246154785\n",
      "Epoch: 4, Step: 40, loss_token: 4.32017707824707\n",
      "Epoch: 4, Step: 41, loss_token: 4.420242786407471\n",
      "Epoch: 4, Step: 42, loss_token: 4.47848653793335\n",
      "Epoch: 4, Step: 43, loss_token: 4.556065559387207\n",
      "Epoch: 4, Step: 44, loss_token: 4.474362373352051\n",
      "Epoch: 4, Step: 45, loss_token: 4.427887439727783\n",
      "Epoch: 4, Step: 46, loss_token: 4.221167087554932\n",
      "Epoch: 4, Step: 47, loss_token: 4.495240688323975\n",
      "Epoch: 4, Step: 48, loss_token: 4.419901371002197\n",
      "Epoch: 4, Step: 49, loss_token: 4.28462553024292\n",
      "Epoch: 4, Step: 50, loss_token: 4.378118515014648\n",
      "Epoch: 4, Step: 51, loss_token: 4.399442195892334\n",
      "Epoch: 4, Step: 52, loss_token: 4.489048480987549\n",
      "Epoch: 4, Step: 53, loss_token: 4.363449573516846\n",
      "Epoch: 4, Step: 54, loss_token: 4.344186782836914\n",
      "Epoch: 4, Step: 55, loss_token: 4.429988384246826\n",
      "Epoch: 4, Step: 56, loss_token: 4.326648235321045\n",
      "Epoch: 4, Step: 57, loss_token: 4.439957141876221\n",
      "Epoch: 4, Step: 58, loss_token: 4.353790283203125\n",
      "Epoch: 4, Step: 59, loss_token: 4.301206588745117\n",
      "Epoch: 4, Step: 60, loss_token: 4.353273391723633\n",
      "Epoch: 4, Step: 61, loss_token: 4.333954334259033\n",
      "Epoch: 4, Step: 62, loss_token: 4.381585597991943\n",
      "Epoch: 4, Step: 63, loss_token: 4.3960442543029785\n",
      "Epoch: 4, Step: 64, loss_token: 4.367011070251465\n",
      "Epoch: 4, Step: 65, loss_token: 4.307089805603027\n",
      "Epoch: 4, Step: 66, loss_token: 4.359227180480957\n",
      "Epoch: 4, Step: 67, loss_token: 4.274107456207275\n",
      "Epoch: 4, Step: 68, loss_token: 4.4514851570129395\n",
      "Epoch: 4, Step: 69, loss_token: 4.410697937011719\n",
      "Epoch: 4, Step: 70, loss_token: 4.373206615447998\n",
      "Epoch: 4, Step: 71, loss_token: 4.377050399780273\n",
      "Epoch: 4, Step: 72, loss_token: 4.4343461990356445\n",
      "Epoch: 4, Step: 73, loss_token: 4.380939960479736\n",
      "Epoch: 4, Step: 74, loss_token: 4.391458034515381\n",
      "Epoch: 4, Step: 75, loss_token: 4.3631134033203125\n",
      "Epoch: 4, Average Loss: 4.396801038792259\n",
      "Validation: 2, Average Loss: 4.229697608947754\n",
      "Model saved\n",
      "Epoch: 5, Step: 0, loss_token: 4.444764614105225\n",
      "Epoch: 5, Step: 1, loss_token: 4.415627479553223\n",
      "Epoch: 5, Step: 2, loss_token: 4.445054531097412\n",
      "Epoch: 5, Step: 3, loss_token: 4.438746452331543\n",
      "Epoch: 5, Step: 4, loss_token: 4.3496809005737305\n",
      "Epoch: 5, Step: 5, loss_token: 4.456954479217529\n",
      "Epoch: 5, Step: 6, loss_token: 4.423211574554443\n",
      "Epoch: 5, Step: 7, loss_token: 4.296404838562012\n",
      "Epoch: 5, Step: 8, loss_token: 4.504792213439941\n",
      "Epoch: 5, Step: 9, loss_token: 4.382071018218994\n",
      "Epoch: 5, Step: 10, loss_token: 4.3933610916137695\n",
      "Epoch: 5, Step: 11, loss_token: 4.318514347076416\n",
      "Epoch: 5, Step: 12, loss_token: 4.481873035430908\n",
      "Epoch: 5, Step: 13, loss_token: 4.401989936828613\n",
      "Epoch: 5, Step: 14, loss_token: 4.340890407562256\n",
      "Epoch: 5, Step: 15, loss_token: 4.366130352020264\n",
      "Epoch: 5, Step: 16, loss_token: 4.388625621795654\n",
      "Epoch: 5, Step: 17, loss_token: 4.423674583435059\n",
      "Epoch: 5, Step: 18, loss_token: 4.464808464050293\n",
      "Epoch: 5, Step: 19, loss_token: 4.40838623046875\n",
      "Epoch: 5, Step: 20, loss_token: 4.410680770874023\n",
      "Epoch: 5, Step: 21, loss_token: 4.3157782554626465\n",
      "Epoch: 5, Step: 22, loss_token: 4.396148204803467\n",
      "Epoch: 5, Step: 23, loss_token: 4.32672119140625\n",
      "Epoch: 5, Step: 24, loss_token: 4.30933952331543\n",
      "Epoch: 5, Step: 25, loss_token: 4.387733459472656\n",
      "Epoch: 5, Step: 26, loss_token: 4.466385364532471\n",
      "Epoch: 5, Step: 27, loss_token: 4.352110385894775\n",
      "Epoch: 5, Step: 28, loss_token: 4.397917747497559\n",
      "Epoch: 5, Step: 29, loss_token: 4.485659599304199\n",
      "Epoch: 5, Step: 30, loss_token: 4.485222339630127\n",
      "Epoch: 5, Step: 31, loss_token: 4.395898818969727\n",
      "Epoch: 5, Step: 32, loss_token: 4.3064351081848145\n",
      "Epoch: 5, Step: 33, loss_token: 4.494626998901367\n",
      "Epoch: 5, Step: 34, loss_token: 4.509975910186768\n",
      "Epoch: 5, Step: 35, loss_token: 4.384511470794678\n",
      "Epoch: 5, Step: 36, loss_token: 4.4453277587890625\n",
      "Epoch: 5, Step: 37, loss_token: 4.412821292877197\n",
      "Epoch: 5, Step: 38, loss_token: 4.326657772064209\n",
      "Epoch: 5, Step: 39, loss_token: 4.44660758972168\n",
      "Epoch: 5, Step: 40, loss_token: 4.389047622680664\n",
      "Epoch: 5, Step: 41, loss_token: 4.357437610626221\n",
      "Epoch: 5, Step: 42, loss_token: 4.268270969390869\n",
      "Epoch: 5, Step: 43, loss_token: 4.347573757171631\n",
      "Epoch: 5, Step: 44, loss_token: 4.40381383895874\n",
      "Epoch: 5, Step: 45, loss_token: 4.461050987243652\n",
      "Epoch: 5, Step: 46, loss_token: 4.395833969116211\n",
      "Epoch: 5, Step: 47, loss_token: 4.3361430168151855\n",
      "Epoch: 5, Step: 48, loss_token: 4.3165602684021\n",
      "Epoch: 5, Step: 49, loss_token: 4.482584476470947\n",
      "Epoch: 5, Step: 50, loss_token: 4.43075704574585\n",
      "Epoch: 5, Step: 51, loss_token: 4.2540788650512695\n",
      "Epoch: 5, Step: 52, loss_token: 4.346852779388428\n",
      "Epoch: 5, Step: 53, loss_token: 4.375403881072998\n",
      "Epoch: 5, Step: 54, loss_token: 4.329732894897461\n",
      "Epoch: 5, Step: 55, loss_token: 4.443234920501709\n",
      "Epoch: 5, Step: 56, loss_token: 4.260083198547363\n",
      "Epoch: 5, Step: 57, loss_token: 4.399266719818115\n",
      "Epoch: 5, Step: 58, loss_token: 4.334600448608398\n",
      "Epoch: 5, Step: 59, loss_token: 4.498690128326416\n",
      "Epoch: 5, Step: 60, loss_token: 4.438236713409424\n",
      "Epoch: 5, Step: 61, loss_token: 4.290369987487793\n",
      "Epoch: 5, Step: 62, loss_token: 4.436201572418213\n",
      "Epoch: 5, Step: 63, loss_token: 4.259307861328125\n",
      "Epoch: 5, Step: 64, loss_token: 4.396238327026367\n",
      "Epoch: 5, Step: 65, loss_token: 4.402503967285156\n",
      "Epoch: 5, Step: 66, loss_token: 4.313161849975586\n",
      "Epoch: 5, Step: 67, loss_token: 4.485743522644043\n",
      "Epoch: 5, Step: 68, loss_token: 4.358806133270264\n",
      "Epoch: 5, Step: 69, loss_token: 4.431059837341309\n",
      "Epoch: 5, Step: 70, loss_token: 4.396010875701904\n",
      "Epoch: 5, Step: 71, loss_token: 4.3646697998046875\n",
      "Epoch: 5, Step: 72, loss_token: 4.387168884277344\n",
      "Epoch: 5, Step: 73, loss_token: 4.201735019683838\n",
      "Epoch: 5, Step: 74, loss_token: 4.347848415374756\n",
      "Epoch: 5, Step: 75, loss_token: 4.393853664398193\n",
      "Epoch: 5, Average Loss: 4.3886323100642155\n",
      "Validation: 2, Average Loss: 4.2581151008605955\n",
      "Epoch: 6, Step: 0, loss_token: 4.388025760650635\n",
      "Epoch: 6, Step: 1, loss_token: 4.300499439239502\n",
      "Epoch: 6, Step: 2, loss_token: 4.355597019195557\n",
      "Epoch: 6, Step: 3, loss_token: 4.452237606048584\n",
      "Epoch: 6, Step: 4, loss_token: 4.381540775299072\n",
      "Epoch: 6, Step: 5, loss_token: 4.32914924621582\n",
      "Epoch: 6, Step: 6, loss_token: 4.325604438781738\n",
      "Epoch: 6, Step: 7, loss_token: 4.39409065246582\n",
      "Epoch: 6, Step: 8, loss_token: 4.403716564178467\n",
      "Epoch: 6, Step: 9, loss_token: 4.301849842071533\n",
      "Epoch: 6, Step: 10, loss_token: 4.335853099822998\n",
      "Epoch: 6, Step: 11, loss_token: 4.3915605545043945\n",
      "Epoch: 6, Step: 12, loss_token: 4.335662364959717\n",
      "Epoch: 6, Step: 13, loss_token: 4.371895790100098\n",
      "Epoch: 6, Step: 14, loss_token: 4.413830280303955\n",
      "Epoch: 6, Step: 15, loss_token: 4.3982415199279785\n",
      "Epoch: 6, Step: 16, loss_token: 4.268764019012451\n",
      "Epoch: 6, Step: 17, loss_token: 4.45704984664917\n",
      "Epoch: 6, Step: 18, loss_token: 4.31673002243042\n",
      "Epoch: 6, Step: 19, loss_token: 4.307520389556885\n",
      "Epoch: 6, Step: 20, loss_token: 4.429232597351074\n",
      "Epoch: 6, Step: 21, loss_token: 4.323320388793945\n",
      "Epoch: 6, Step: 22, loss_token: 4.293245315551758\n",
      "Epoch: 6, Step: 23, loss_token: 4.328080177307129\n",
      "Epoch: 6, Step: 24, loss_token: 4.313232421875\n",
      "Epoch: 6, Step: 25, loss_token: 4.482818603515625\n",
      "Epoch: 6, Step: 26, loss_token: 4.402547359466553\n",
      "Epoch: 6, Step: 27, loss_token: 4.3087921142578125\n",
      "Epoch: 6, Step: 28, loss_token: 4.4089436531066895\n",
      "Epoch: 6, Step: 29, loss_token: 4.2580180168151855\n",
      "Epoch: 6, Step: 30, loss_token: 4.31685209274292\n",
      "Epoch: 6, Step: 31, loss_token: 4.428206920623779\n",
      "Epoch: 6, Step: 32, loss_token: 4.212823867797852\n",
      "Epoch: 6, Step: 33, loss_token: 4.345427989959717\n",
      "Epoch: 6, Step: 34, loss_token: 4.4491353034973145\n",
      "Epoch: 6, Step: 35, loss_token: 4.330878734588623\n",
      "Epoch: 6, Step: 36, loss_token: 4.412661552429199\n",
      "Epoch: 6, Step: 37, loss_token: 4.329758167266846\n",
      "Epoch: 6, Step: 38, loss_token: 4.460498332977295\n",
      "Epoch: 6, Step: 39, loss_token: 4.4363203048706055\n",
      "Epoch: 6, Step: 40, loss_token: 4.354048728942871\n",
      "Epoch: 6, Step: 41, loss_token: 4.355766773223877\n",
      "Epoch: 6, Step: 42, loss_token: 4.330410957336426\n",
      "Epoch: 6, Step: 43, loss_token: 4.333415508270264\n",
      "Epoch: 6, Step: 44, loss_token: 4.32478141784668\n",
      "Epoch: 6, Step: 45, loss_token: 4.353209972381592\n",
      "Epoch: 6, Step: 46, loss_token: 4.345292568206787\n",
      "Epoch: 6, Step: 47, loss_token: 4.424276828765869\n",
      "Epoch: 6, Step: 48, loss_token: 4.374309062957764\n",
      "Epoch: 6, Step: 49, loss_token: 4.347501277923584\n",
      "Epoch: 6, Step: 50, loss_token: 4.390013694763184\n",
      "Epoch: 6, Step: 51, loss_token: 4.300099849700928\n",
      "Epoch: 6, Step: 52, loss_token: 4.255922794342041\n",
      "Epoch: 6, Step: 53, loss_token: 4.3634562492370605\n",
      "Epoch: 6, Step: 54, loss_token: 4.355169296264648\n",
      "Epoch: 6, Step: 55, loss_token: 4.353207588195801\n",
      "Epoch: 6, Step: 56, loss_token: 4.2969489097595215\n",
      "Epoch: 6, Step: 57, loss_token: 4.3456220626831055\n",
      "Epoch: 6, Step: 58, loss_token: 4.338735103607178\n",
      "Epoch: 6, Step: 59, loss_token: 4.302908897399902\n",
      "Epoch: 6, Step: 60, loss_token: 4.365840435028076\n",
      "Epoch: 6, Step: 61, loss_token: 4.370529651641846\n",
      "Epoch: 6, Step: 62, loss_token: 4.323709487915039\n",
      "Epoch: 6, Step: 63, loss_token: 4.542630195617676\n",
      "Epoch: 6, Step: 64, loss_token: 4.380983829498291\n",
      "Epoch: 6, Step: 65, loss_token: 4.348427772521973\n",
      "Epoch: 6, Step: 66, loss_token: 4.438653469085693\n",
      "Epoch: 6, Step: 67, loss_token: 4.267820835113525\n",
      "Epoch: 6, Step: 68, loss_token: 4.2944769859313965\n",
      "Epoch: 6, Step: 69, loss_token: 4.325877666473389\n",
      "Epoch: 6, Step: 70, loss_token: 4.316140174865723\n",
      "Epoch: 6, Step: 71, loss_token: 4.382351398468018\n",
      "Epoch: 6, Step: 72, loss_token: 4.32155179977417\n",
      "Epoch: 6, Step: 73, loss_token: 4.262452125549316\n",
      "Epoch: 6, Step: 74, loss_token: 4.30291223526001\n",
      "Epoch: 6, Step: 75, loss_token: 4.391085624694824\n",
      "Epoch: 6, Average Loss: 4.355009925992865\n",
      "Validation: 2, Average Loss: 4.224684333801269\n",
      "Model saved\n",
      "Epoch: 7, Step: 0, loss_token: 4.319986820220947\n",
      "Epoch: 7, Step: 1, loss_token: 4.3383989334106445\n",
      "Epoch: 7, Step: 2, loss_token: 4.363893985748291\n",
      "Epoch: 7, Step: 3, loss_token: 4.385978698730469\n",
      "Epoch: 7, Step: 4, loss_token: 4.316901206970215\n",
      "Epoch: 7, Step: 5, loss_token: 4.271430969238281\n",
      "Epoch: 7, Step: 6, loss_token: 4.331873416900635\n",
      "Epoch: 7, Step: 7, loss_token: 4.406170845031738\n",
      "Epoch: 7, Step: 8, loss_token: 4.314018249511719\n",
      "Epoch: 7, Step: 9, loss_token: 4.399956226348877\n",
      "Epoch: 7, Step: 10, loss_token: 4.3498101234436035\n",
      "Epoch: 7, Step: 11, loss_token: 4.457406997680664\n",
      "Epoch: 7, Step: 12, loss_token: 4.437812328338623\n",
      "Epoch: 7, Step: 13, loss_token: 4.3263044357299805\n",
      "Epoch: 7, Step: 14, loss_token: 4.39695930480957\n",
      "Epoch: 7, Step: 15, loss_token: 4.350430011749268\n",
      "Epoch: 7, Step: 16, loss_token: 4.179388523101807\n",
      "Epoch: 7, Step: 17, loss_token: 4.481569766998291\n",
      "Epoch: 7, Step: 18, loss_token: 4.256503105163574\n",
      "Epoch: 7, Step: 19, loss_token: 4.286709785461426\n",
      "Epoch: 7, Step: 20, loss_token: 4.322539806365967\n",
      "Epoch: 7, Step: 21, loss_token: 4.356291770935059\n",
      "Epoch: 7, Step: 22, loss_token: 4.221529483795166\n",
      "Epoch: 7, Step: 23, loss_token: 4.349589824676514\n",
      "Epoch: 7, Step: 24, loss_token: 4.234426975250244\n",
      "Epoch: 7, Step: 25, loss_token: 4.264524936676025\n",
      "Epoch: 7, Step: 26, loss_token: 4.2862229347229\n",
      "Epoch: 7, Step: 27, loss_token: 4.356302738189697\n",
      "Epoch: 7, Step: 28, loss_token: 4.333864688873291\n",
      "Epoch: 7, Step: 29, loss_token: 4.360551357269287\n",
      "Epoch: 7, Step: 30, loss_token: 4.211220741271973\n",
      "Epoch: 7, Step: 31, loss_token: 4.425443649291992\n",
      "Epoch: 7, Step: 32, loss_token: 4.327090263366699\n",
      "Epoch: 7, Step: 33, loss_token: 4.322797775268555\n",
      "Epoch: 7, Step: 34, loss_token: 4.346761226654053\n",
      "Epoch: 7, Step: 35, loss_token: 4.376881122589111\n",
      "Epoch: 7, Step: 36, loss_token: 4.371855735778809\n",
      "Epoch: 7, Step: 37, loss_token: 4.387060165405273\n",
      "Epoch: 7, Step: 38, loss_token: 4.286184310913086\n",
      "Epoch: 7, Step: 39, loss_token: 4.335851192474365\n",
      "Epoch: 7, Step: 40, loss_token: 4.285861015319824\n",
      "Epoch: 7, Step: 41, loss_token: 4.333043098449707\n",
      "Epoch: 7, Step: 42, loss_token: 4.282504558563232\n",
      "Epoch: 7, Step: 43, loss_token: 4.31166410446167\n",
      "Epoch: 7, Step: 44, loss_token: 4.362050533294678\n",
      "Epoch: 7, Step: 45, loss_token: 4.312767505645752\n",
      "Epoch: 7, Step: 46, loss_token: 4.269869327545166\n",
      "Epoch: 7, Step: 47, loss_token: 4.341446399688721\n",
      "Epoch: 7, Step: 48, loss_token: 4.31264591217041\n",
      "Epoch: 7, Step: 49, loss_token: 4.3282551765441895\n",
      "Epoch: 7, Step: 50, loss_token: 4.363117694854736\n",
      "Epoch: 7, Step: 51, loss_token: 4.315422534942627\n",
      "Epoch: 7, Step: 52, loss_token: 4.324651718139648\n",
      "Epoch: 7, Step: 53, loss_token: 4.351363182067871\n",
      "Epoch: 7, Step: 54, loss_token: 4.326542377471924\n",
      "Epoch: 7, Step: 55, loss_token: 4.333471775054932\n",
      "Epoch: 7, Step: 56, loss_token: 4.268320083618164\n",
      "Epoch: 7, Step: 57, loss_token: 4.465639591217041\n",
      "Epoch: 7, Step: 58, loss_token: 4.293604850769043\n",
      "Epoch: 7, Step: 59, loss_token: 4.347373962402344\n",
      "Epoch: 7, Step: 60, loss_token: 4.368955135345459\n",
      "Epoch: 7, Step: 61, loss_token: 4.414141654968262\n",
      "Epoch: 7, Step: 62, loss_token: 4.376830577850342\n",
      "Epoch: 7, Step: 63, loss_token: 4.296624660491943\n",
      "Epoch: 7, Step: 64, loss_token: 4.4589362144470215\n",
      "Epoch: 7, Step: 65, loss_token: 4.178220272064209\n",
      "Epoch: 7, Step: 66, loss_token: 4.3075666427612305\n",
      "Epoch: 7, Step: 67, loss_token: 4.352646350860596\n",
      "Epoch: 7, Step: 68, loss_token: 4.292713642120361\n",
      "Epoch: 7, Step: 69, loss_token: 4.343414306640625\n",
      "Epoch: 7, Step: 70, loss_token: 4.360589504241943\n",
      "Epoch: 7, Step: 71, loss_token: 4.2686333656311035\n",
      "Epoch: 7, Step: 72, loss_token: 4.346166610717773\n",
      "Epoch: 7, Step: 73, loss_token: 4.312132835388184\n",
      "Epoch: 7, Step: 74, loss_token: 4.356995582580566\n",
      "Epoch: 7, Step: 75, loss_token: 4.383936405181885\n",
      "Epoch: 7, Average Loss: 4.334165968393025\n",
      "Validation: 2, Average Loss: 4.246333312988281\n",
      "Epoch: 8, Step: 0, loss_token: 4.416202068328857\n",
      "Epoch: 8, Step: 1, loss_token: 4.476073741912842\n",
      "Epoch: 8, Step: 2, loss_token: 4.376140117645264\n",
      "Epoch: 8, Step: 3, loss_token: 4.410070896148682\n",
      "Epoch: 8, Step: 4, loss_token: 4.325881004333496\n",
      "Epoch: 8, Step: 5, loss_token: 4.307260990142822\n",
      "Epoch: 8, Step: 6, loss_token: 4.308884143829346\n",
      "Epoch: 8, Step: 7, loss_token: 4.38323450088501\n",
      "Epoch: 8, Step: 8, loss_token: 4.373344898223877\n",
      "Epoch: 8, Step: 9, loss_token: 4.345769882202148\n",
      "Epoch: 8, Step: 10, loss_token: 4.271397113800049\n",
      "Epoch: 8, Step: 11, loss_token: 4.357150554656982\n",
      "Epoch: 8, Step: 12, loss_token: 4.3229594230651855\n",
      "Epoch: 8, Step: 13, loss_token: 4.2763776779174805\n",
      "Epoch: 8, Step: 14, loss_token: 4.3811821937561035\n",
      "Epoch: 8, Step: 15, loss_token: 4.312224388122559\n",
      "Epoch: 8, Step: 16, loss_token: 4.410888195037842\n",
      "Epoch: 8, Step: 17, loss_token: 4.244030475616455\n",
      "Epoch: 8, Step: 18, loss_token: 4.244203567504883\n",
      "Epoch: 8, Step: 19, loss_token: 4.236083507537842\n",
      "Epoch: 8, Step: 20, loss_token: 4.230581283569336\n",
      "Epoch: 8, Step: 21, loss_token: 4.2428059577941895\n",
      "Epoch: 8, Step: 22, loss_token: 4.365111351013184\n",
      "Epoch: 8, Step: 23, loss_token: 4.4016265869140625\n",
      "Epoch: 8, Step: 24, loss_token: 4.260987758636475\n",
      "Epoch: 8, Step: 25, loss_token: 4.409463882446289\n",
      "Epoch: 8, Step: 26, loss_token: 4.306197643280029\n",
      "Epoch: 8, Step: 27, loss_token: 4.192428112030029\n",
      "Epoch: 8, Step: 28, loss_token: 4.385110855102539\n",
      "Epoch: 8, Step: 29, loss_token: 4.31702995300293\n",
      "Epoch: 8, Step: 30, loss_token: 4.413722038269043\n",
      "Epoch: 8, Step: 31, loss_token: 4.36562442779541\n",
      "Epoch: 8, Step: 32, loss_token: 4.356192588806152\n",
      "Epoch: 8, Step: 33, loss_token: 4.427928447723389\n",
      "Epoch: 8, Step: 34, loss_token: 4.239137172698975\n",
      "Epoch: 8, Step: 35, loss_token: 4.402403354644775\n",
      "Epoch: 8, Step: 36, loss_token: 4.267133712768555\n",
      "Epoch: 8, Step: 37, loss_token: 4.385004997253418\n",
      "Epoch: 8, Step: 38, loss_token: 4.298508644104004\n",
      "Epoch: 8, Step: 39, loss_token: 4.261621475219727\n",
      "Epoch: 8, Step: 40, loss_token: 4.431042671203613\n",
      "Epoch: 8, Step: 41, loss_token: 4.278922080993652\n",
      "Epoch: 8, Step: 42, loss_token: 4.31575345993042\n",
      "Epoch: 8, Step: 43, loss_token: 4.459787845611572\n",
      "Epoch: 8, Step: 44, loss_token: 4.345344543457031\n",
      "Epoch: 8, Step: 45, loss_token: 4.27762508392334\n",
      "Epoch: 8, Step: 46, loss_token: 4.366709232330322\n",
      "Epoch: 8, Step: 47, loss_token: 4.272273063659668\n",
      "Epoch: 8, Step: 48, loss_token: 4.2981133460998535\n",
      "Epoch: 8, Step: 49, loss_token: 4.369169235229492\n",
      "Epoch: 8, Step: 50, loss_token: 4.30866003036499\n",
      "Epoch: 8, Step: 51, loss_token: 4.241755962371826\n",
      "Epoch: 8, Step: 52, loss_token: 4.3044843673706055\n",
      "Epoch: 8, Step: 53, loss_token: 4.362344741821289\n",
      "Epoch: 8, Step: 54, loss_token: 4.341554641723633\n",
      "Epoch: 8, Step: 55, loss_token: 4.218626976013184\n",
      "Epoch: 8, Step: 56, loss_token: 4.337549686431885\n",
      "Epoch: 8, Step: 57, loss_token: 4.383703708648682\n",
      "Epoch: 8, Step: 58, loss_token: 4.317121505737305\n",
      "Epoch: 8, Step: 59, loss_token: 4.377871036529541\n",
      "Epoch: 8, Step: 60, loss_token: 4.3838090896606445\n",
      "Epoch: 8, Step: 61, loss_token: 4.266016006469727\n",
      "Epoch: 8, Step: 62, loss_token: 4.390610694885254\n",
      "Epoch: 8, Step: 63, loss_token: 4.379519939422607\n",
      "Epoch: 8, Step: 64, loss_token: 4.2722907066345215\n",
      "Epoch: 8, Step: 65, loss_token: 4.348655700683594\n",
      "Epoch: 8, Step: 66, loss_token: 4.246758937835693\n",
      "Epoch: 8, Step: 67, loss_token: 4.179938793182373\n",
      "Epoch: 8, Step: 68, loss_token: 4.233145236968994\n",
      "Epoch: 8, Step: 69, loss_token: 4.242196083068848\n",
      "Epoch: 8, Step: 70, loss_token: 4.454292297363281\n",
      "Epoch: 8, Step: 71, loss_token: 4.263070583343506\n",
      "Epoch: 8, Step: 72, loss_token: 4.459975242614746\n",
      "Epoch: 8, Step: 73, loss_token: 4.22914457321167\n",
      "Epoch: 8, Step: 74, loss_token: 4.24576997756958\n",
      "Epoch: 8, Step: 75, loss_token: 4.371301651000977\n",
      "Epoch: 8, Average Loss: 4.327406425225107\n",
      "Validation: 2, Average Loss: 4.266350364685058\n",
      "Epoch: 9, Step: 0, loss_token: 4.342756748199463\n",
      "Epoch: 9, Step: 1, loss_token: 4.297422885894775\n",
      "Epoch: 9, Step: 2, loss_token: 4.355051517486572\n",
      "Epoch: 9, Step: 3, loss_token: 4.1324262619018555\n",
      "Epoch: 9, Step: 4, loss_token: 4.326771259307861\n",
      "Epoch: 9, Step: 5, loss_token: 4.268476486206055\n",
      "Epoch: 9, Step: 6, loss_token: 4.335931301116943\n",
      "Epoch: 9, Step: 7, loss_token: 4.316836833953857\n",
      "Epoch: 9, Step: 8, loss_token: 4.363773822784424\n",
      "Epoch: 9, Step: 9, loss_token: 4.334871292114258\n",
      "Epoch: 9, Step: 10, loss_token: 4.3472819328308105\n",
      "Epoch: 9, Step: 11, loss_token: 4.335177421569824\n",
      "Epoch: 9, Step: 12, loss_token: 4.359893798828125\n",
      "Epoch: 9, Step: 13, loss_token: 4.28215217590332\n",
      "Epoch: 9, Step: 14, loss_token: 4.382121562957764\n",
      "Epoch: 9, Step: 15, loss_token: 4.217968940734863\n",
      "Epoch: 9, Step: 16, loss_token: 4.392524242401123\n",
      "Epoch: 9, Step: 17, loss_token: 4.359269142150879\n",
      "Epoch: 9, Step: 18, loss_token: 4.236900806427002\n",
      "Epoch: 9, Step: 19, loss_token: 4.2711710929870605\n",
      "Epoch: 9, Step: 20, loss_token: 4.257296085357666\n",
      "Epoch: 9, Step: 21, loss_token: 4.296218395233154\n",
      "Epoch: 9, Step: 22, loss_token: 4.392695903778076\n",
      "Epoch: 9, Step: 23, loss_token: 4.229411602020264\n",
      "Epoch: 9, Step: 24, loss_token: 4.248098850250244\n",
      "Epoch: 9, Step: 25, loss_token: 4.345149517059326\n",
      "Epoch: 9, Step: 26, loss_token: 4.196813106536865\n",
      "Epoch: 9, Step: 27, loss_token: 4.294539451599121\n",
      "Epoch: 9, Step: 28, loss_token: 4.41933536529541\n",
      "Epoch: 9, Step: 29, loss_token: 4.245964050292969\n",
      "Epoch: 9, Step: 30, loss_token: 4.419702529907227\n",
      "Epoch: 9, Step: 31, loss_token: 4.267027378082275\n",
      "Epoch: 9, Step: 32, loss_token: 4.236538887023926\n",
      "Epoch: 9, Step: 33, loss_token: 4.271028995513916\n",
      "Epoch: 9, Step: 34, loss_token: 4.262033939361572\n",
      "Epoch: 9, Step: 35, loss_token: 4.240922927856445\n",
      "Epoch: 9, Step: 36, loss_token: 4.266035079956055\n",
      "Epoch: 9, Step: 37, loss_token: 4.311335563659668\n",
      "Epoch: 9, Step: 38, loss_token: 4.4432878494262695\n",
      "Epoch: 9, Step: 39, loss_token: 4.288033962249756\n",
      "Epoch: 9, Step: 40, loss_token: 4.353302001953125\n",
      "Epoch: 9, Step: 41, loss_token: 4.353992938995361\n",
      "Epoch: 9, Step: 42, loss_token: 4.344676494598389\n",
      "Epoch: 9, Step: 43, loss_token: 4.175295829772949\n",
      "Epoch: 9, Step: 44, loss_token: 4.332980632781982\n",
      "Epoch: 9, Step: 45, loss_token: 4.266259670257568\n",
      "Epoch: 9, Step: 46, loss_token: 4.297903060913086\n",
      "Epoch: 9, Step: 47, loss_token: 4.242495059967041\n",
      "Epoch: 9, Step: 48, loss_token: 4.234464645385742\n",
      "Epoch: 9, Step: 49, loss_token: 4.2460103034973145\n",
      "Epoch: 9, Step: 50, loss_token: 4.293988227844238\n",
      "Epoch: 9, Step: 51, loss_token: 4.1050286293029785\n",
      "Epoch: 9, Step: 52, loss_token: 4.317713737487793\n",
      "Epoch: 9, Step: 53, loss_token: 4.314772605895996\n",
      "Epoch: 9, Step: 54, loss_token: 4.378973960876465\n",
      "Epoch: 9, Step: 55, loss_token: 4.286179065704346\n",
      "Epoch: 9, Step: 56, loss_token: 4.2517619132995605\n",
      "Epoch: 9, Step: 57, loss_token: 4.2423224449157715\n",
      "Epoch: 9, Step: 58, loss_token: 4.273897171020508\n",
      "Epoch: 9, Step: 59, loss_token: 4.304295539855957\n",
      "Epoch: 9, Step: 60, loss_token: 4.16524600982666\n",
      "Epoch: 9, Step: 61, loss_token: 4.326941013336182\n",
      "Epoch: 9, Step: 62, loss_token: 4.313439846038818\n",
      "Epoch: 9, Step: 63, loss_token: 4.2952728271484375\n",
      "Epoch: 9, Step: 64, loss_token: 4.417820930480957\n",
      "Epoch: 9, Step: 65, loss_token: 4.331014633178711\n",
      "Epoch: 9, Step: 66, loss_token: 4.265707492828369\n",
      "Epoch: 9, Step: 67, loss_token: 4.27057409286499\n",
      "Epoch: 9, Step: 68, loss_token: 4.224128246307373\n",
      "Epoch: 9, Step: 69, loss_token: 4.235561847686768\n",
      "Epoch: 9, Step: 70, loss_token: 4.3183465003967285\n",
      "Epoch: 9, Step: 71, loss_token: 4.377070426940918\n",
      "Epoch: 9, Step: 72, loss_token: 4.223078727722168\n",
      "Epoch: 9, Step: 73, loss_token: 4.279299259185791\n",
      "Epoch: 9, Step: 74, loss_token: 4.26320219039917\n",
      "Epoch: 9, Step: 75, loss_token: 4.335991859436035\n",
      "Epoch: 9, Average Loss: 4.295384984267385\n",
      "Validation: 2, Average Loss: 4.260885429382324\n",
      "Epoch: 10, Step: 0, loss_token: 4.217940807342529\n",
      "Epoch: 10, Step: 1, loss_token: 4.237050533294678\n",
      "Epoch: 10, Step: 2, loss_token: 4.32341194152832\n",
      "Epoch: 10, Step: 3, loss_token: 4.262950420379639\n",
      "Epoch: 10, Step: 4, loss_token: 4.2273969650268555\n",
      "Epoch: 10, Step: 5, loss_token: 4.21999454498291\n",
      "Epoch: 10, Step: 6, loss_token: 4.226181983947754\n",
      "Epoch: 10, Step: 7, loss_token: 4.369457721710205\n",
      "Epoch: 10, Step: 8, loss_token: 4.423025131225586\n",
      "Epoch: 10, Step: 9, loss_token: 4.261409282684326\n",
      "Epoch: 10, Step: 10, loss_token: 4.209415435791016\n",
      "Epoch: 10, Step: 11, loss_token: 4.407351970672607\n",
      "Epoch: 10, Step: 12, loss_token: 4.176237106323242\n",
      "Epoch: 10, Step: 13, loss_token: 4.31301736831665\n",
      "Epoch: 10, Step: 14, loss_token: 4.248056888580322\n",
      "Epoch: 10, Step: 15, loss_token: 4.270041465759277\n",
      "Epoch: 10, Step: 16, loss_token: 4.290310382843018\n",
      "Epoch: 10, Step: 17, loss_token: 4.2952799797058105\n",
      "Epoch: 10, Step: 18, loss_token: 4.283862590789795\n",
      "Epoch: 10, Step: 19, loss_token: 4.299740314483643\n",
      "Epoch: 10, Step: 20, loss_token: 4.319552898406982\n",
      "Epoch: 10, Step: 21, loss_token: 4.2010602951049805\n",
      "Epoch: 10, Step: 22, loss_token: 4.309995174407959\n",
      "Epoch: 10, Step: 23, loss_token: 4.200984001159668\n",
      "Epoch: 10, Step: 24, loss_token: 4.283381462097168\n",
      "Epoch: 10, Step: 25, loss_token: 4.1017866134643555\n",
      "Epoch: 10, Step: 26, loss_token: 4.296475410461426\n",
      "Epoch: 10, Step: 27, loss_token: 4.210101127624512\n",
      "Epoch: 10, Step: 28, loss_token: 4.296638488769531\n",
      "Epoch: 10, Step: 29, loss_token: 4.312429904937744\n",
      "Epoch: 10, Step: 30, loss_token: 4.306090831756592\n",
      "Epoch: 10, Step: 31, loss_token: 4.199275493621826\n",
      "Epoch: 10, Step: 32, loss_token: 4.272574424743652\n",
      "Epoch: 10, Step: 33, loss_token: 4.285712718963623\n",
      "Epoch: 10, Step: 34, loss_token: 4.235446453094482\n",
      "Epoch: 10, Step: 35, loss_token: 4.288981914520264\n",
      "Epoch: 10, Step: 36, loss_token: 4.174615859985352\n",
      "Epoch: 10, Step: 37, loss_token: 4.2286272048950195\n",
      "Epoch: 10, Step: 38, loss_token: 4.286884784698486\n",
      "Epoch: 10, Step: 39, loss_token: 4.279839992523193\n",
      "Epoch: 10, Step: 40, loss_token: 4.36423921585083\n",
      "Epoch: 10, Step: 41, loss_token: 4.164549827575684\n",
      "Epoch: 10, Step: 42, loss_token: 4.225014686584473\n",
      "Epoch: 10, Step: 43, loss_token: 4.20446252822876\n",
      "Epoch: 10, Step: 44, loss_token: 4.2354841232299805\n",
      "Epoch: 10, Step: 45, loss_token: 4.2195634841918945\n",
      "Epoch: 10, Step: 46, loss_token: 4.196032524108887\n",
      "Epoch: 10, Step: 47, loss_token: 4.246337413787842\n",
      "Epoch: 10, Step: 48, loss_token: 4.293424129486084\n",
      "Epoch: 10, Step: 49, loss_token: 4.236217021942139\n",
      "Epoch: 10, Step: 50, loss_token: 4.32969331741333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 25\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()  \n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss_token: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_token\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "epochs = 10\n",
    "\n",
    "# Define optimizer\n",
    "\n",
    "print(int(1.33*lendata/(batch_size*context_length)))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i in range(0,int(1.33*lendata/(batch_size*context_length))):\n",
    "        # Get batch\n",
    "        X = get_batch('train')\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output, loss_token = model(X.to(device))\n",
    "        loss = loss_token\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()  \n",
    "         \n",
    "        if i % 1 == 0:\n",
    "            print(f\"Epoch: {epoch+1}, Step: {i}, loss_token: {loss_token.item()}\")\n",
    "    \n",
    "    avg_loss = total_loss/int(1.33*lendata/(batch_size*context_length))\n",
    "    print(f\"Epoch: {epoch+1}, Average Loss: {     avg_loss}\")\n",
    "\n",
    "    if epoch > -1:\n",
    "        val_loss = validation(model)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"model.pt\")\n",
    "            print(\"Model saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WorldModel_LLM(16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_37580\\3903519863.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 2, Average Loss: 4.662626457214356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.662626457214356"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will explain the meaning of life. The meaning of life is that our happiness begins with us.of finding our home again.our lifelong poor child who grew up restricted on food alone during recess and have little interest in her childhood childhood children. \"if this last point is too far to burden again, and we could halt installation at this stage without the necessary delays, but we let it grow steadily until we could keep it within our budget goals. 24:00 so we did what we had to do: we changed departments making sure that educators could work together to help students achieve early success.\n",
      "\n",
      "\"One way of improving our solution is to extend the program to include our neighbors on double commutes than we do here.\n",
      "\n",
      "Largest rail service fee in government process in decades in groceries waste-laden baggies. America's hops and potato industry has also been big, and remain major operations, meaning unemployed people now face up to five years of unemployment insurance. the important thing is to all recycler families have to solve a national problem, and we must do we so with a restoration, that we restore the same to us so that we can achieve our shared goal of equal rights for all citizens of countries of the world. for the people everywhere we believe that the truest military achievement is the denial of homelessness and other needs, but not almost everything.\n",
      "\n",
      "4) wvw improved the overall reliability. experienced me patching the pdword and with winsof\n"
     ]
    }
   ],
   "source": [
    "text = \"I will explain the meaning of life. The meaning of life is\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    res = model.generate(input_ids, ntokens=300)\n",
    "    output = tokenizer.decode(res[0].tolist())\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize sentences worresponding to different level 1 embedding values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will explain the meaning of life. The meaning of life is that our happiness begins with us.of finding our home again.our lifelong poor child who grew\n",
      "-----------\n",
      " up restricted on food alone during recess and have little interest in her childhood childhood children. \"if this last point is too far to burden again, and we could\n",
      "-----------\n",
      " halt installation at this stage without the necessary delays, but we let it grow steadily until we could keep it within our budget goals. 24:00 so we did\n",
      "-----------\n",
      " what we had to do: we changed departments making sure that educators could work together to help students achieve early success.\n",
      "\n",
      "\"One way of improving our solution\n",
      "-----------\n",
      " is to extend the program to include our neighbors on double commutes than we do here.\n",
      "\n",
      "Largest rail service fee in government process in decades in\n",
      "-----------\n",
      " groceries waste-laden baggies. America's hops and potato industry has also been big, and remain major operations, meaning unemployed people now face up to five\n",
      "-----------\n",
      " years of unemployment insurance. the important thing is to all recycler families have to solve a national problem, and we must do we so with a restoration, that\n",
      "-----------\n",
      " we restore the same to us so that we can achieve our shared goal of equal rights for all citizens of countries of the world. for the people everywhere we believe\n",
      "-----------\n",
      " that the truest military achievement is the denial of homelessness and other needs, but not almost everything.\n",
      "\n",
      "4) wvw improved the overall reliability.\n",
      "-----------\n",
      " experienced me patching the pdword and with winsof\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "size = 32\n",
    "while i*size < len(res[0]):\n",
    "    print(tokenizer.decode(res[0][i*size:size+i*size].tolist()))\n",
    "    print('-----------')\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a new tokenizer from an old one",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
